{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "colab": {
      "name": "RandomForest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BodhviGaur/Cloud-Flow-Size-Prediction/blob/master/RandomForest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShRdde-MUQFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# KMeans- Random Forest:\n",
        "\n",
        "#All imports declarations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import csv\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "np.random.seed(1)     \n",
        "\n",
        "\n",
        "def add_context(data_df, context = 0):\n",
        "\n",
        "    L = len(data_df)\n",
        "    cols = list(data_df.columns)\n",
        "\n",
        "    context_data = []\n",
        "    \n",
        "    row_1 = np.asarray(data_df.iloc[0,:])\n",
        "    row_1 = np.reshape(row_1,(1,-1))\n",
        "    row_1 = pd.DataFrame(row_1, columns=cols)\n",
        "    #pad data with data[0] context times\n",
        "    data = pd.concat([row_1]*context + [data_df])\n",
        "    \n",
        "    try:\n",
        "        del data[0]\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    data = data.to_numpy()\n",
        "    \n",
        "    \n",
        "    for fi in range(context, L):\n",
        "        frame = data[fi-context:fi+1].flatten()\n",
        "        context_data.append(frame)\n",
        "\n",
        "    return pd.DataFrame(context_data, columns = cols*(context+1))\n",
        "\n",
        "def read_dataset(filename):\n",
        "    dataset = pd.read_csv(filename)\n",
        "    df_cols = list(dataset.columns)\n",
        "    dataset = scaler.fit_transform(dataset)\n",
        "    dataset = pd.DataFrame(dataset, columns = df_cols)\n",
        "    return dataset\n",
        "\n",
        "def get_data_labels(df, context = 0):\n",
        "    label = df['flow_size'][:-context]\n",
        "    try:\n",
        "      del df['job']\n",
        "    except:\n",
        "      pass\n",
        "    df = add_context(df, context)\n",
        "    df.iloc[:, -1] = 0\n",
        "    #add context to labels\n",
        "    return df, label\n",
        "\n",
        "\n",
        "def plot_features(labels,values,title = 'Graph',rot = 90):\n",
        "    index = np.arange(len(labels))\n",
        "    plt.bar(index*2.0,values,width = 1)\n",
        "    plt.xticks(index*2.0, labels, fontsize=15, rotation=rot)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7Lt1Kpp_op8",
        "colab_type": "code",
        "outputId": "055ba1b1-915e-433b-c99d-65c9edc2c2cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "train_df = read_dataset('KMeans_training.csv')\n",
        "test_df = read_dataset('KMeans_test.csv')\n",
        "\n",
        "train_x, train_y = get_data_labels(train_df)\n",
        "test_x, test_y = get_data_labels(test_df)\n",
        "\n",
        "\n",
        "rfc = RandomForestRegressor(n_estimators=19)\n",
        "rfc.fit(train_x, train_y)\n",
        "\n",
        "predict_y = rfc.predict(test_x)\n",
        "print(19, r2_score(test_y, predict_y))\n",
        "\n",
        "'''Results: r2 score:\n",
        "1 0.6153378481333496\n",
        "2 0.7679077114917076\n",
        "3 0.7859023972802381\n",
        "4 0.7624173113467956\n",
        "5 0.8079303407961053\n",
        "6 0.8221153491494789\n",
        "7 0.8018883937126403\n",
        "8 0.8116497085054406\n",
        "9 0.832901877654538\n",
        "10 0.8261256054116369\n",
        "11 0.8270112437215036\n",
        "12 0.8269189559410328\n",
        "13 0.8334078560798294\n",
        "14 0.82346961613714\n",
        "15 0.8145659858799161\n",
        "16 0.8168822359415231\n",
        "17 0.8139808079940153\n",
        "18 0.8234025084881461\n",
        "\n",
        "##19 0.837998744640424\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19 0.8207272748465316\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Results: r2 score:\\n1 0.6153378481333496\\n2 0.7679077114917076\\n3 0.7859023972802381\\n4 0.7624173113467956\\n5 0.8079303407961053\\n6 0.8221153491494789\\n7 0.8018883937126403\\n8 0.8116497085054406\\n9 0.832901877654538\\n10 0.8261256054116369\\n11 0.8270112437215036\\n12 0.8269189559410328\\n13 0.8334078560798294\\n14 0.82346961613714\\n15 0.8145659858799161\\n16 0.8168822359415231\\n17 0.8139808079940153\\n18 0.8234025084881461\\n\\n##19 0.837998744640424\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1JjUy4ogsbn",
        "colab_type": "code",
        "outputId": "f8821434-5591-4180-aae7-c4903cc6eb57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.externals import joblib\n",
        "# Output a pickle file for the model\n",
        "joblib.dump(rfc, 'rfc_KMeans.pkl') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rfc_KMeans.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ntjuz_cUQFO",
        "colab_type": "code",
        "outputId": "51bca652-98f7-4534-ec97-ae0d042155ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#PageRank: Random Forest \n",
        "#All imports declarations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import csv\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "np.random.seed(1)     \n",
        "\n",
        "\n",
        "def add_context(data_df, context = 0):\n",
        "\n",
        "    L = len(data_df)\n",
        "    cols = list(data_df.columns)\n",
        "\n",
        "    context_data = []\n",
        "    \n",
        "    row_1 = np.asarray(data_df.iloc[0,:])\n",
        "    row_1 = np.reshape(row_1,(1,-1))\n",
        "    row_1 = pd.DataFrame(row_1, columns=cols)\n",
        "    #pad data with data[0] context times\n",
        "    data = pd.concat([row_1]*context + [data_df])\n",
        "    \n",
        "    try:\n",
        "        del data[0]\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    data = data.to_numpy()\n",
        "    \n",
        "    \n",
        "    for fi in range(context, L):\n",
        "        frame = data[fi-context:fi+1].flatten()\n",
        "        context_data.append(frame)\n",
        "\n",
        "    return pd.DataFrame(context_data, columns = cols*(context+1))\n",
        "\n",
        "def read_dataset(filename):\n",
        "    dataset = pd.read_csv(filename)\n",
        "    df_cols = list(dataset.columns)\n",
        "    dataset = scaler.fit_transform(dataset)\n",
        "    dataset = pd.DataFrame(dataset, columns = df_cols)\n",
        "    return dataset\n",
        "\n",
        "def get_data_labels(df, context = 0):\n",
        "    label = df['flow_size'][:-context]\n",
        "    try:\n",
        "      del df['job']\n",
        "    except:\n",
        "      pass\n",
        "    df = add_context(df, context)\n",
        "    df.iloc[:, -1] = 0\n",
        "    #add context to labels\n",
        "    return df, label\n",
        "\n",
        "\n",
        "def plot_features(labels,values,title = 'Graph',rot = 90):\n",
        "    index = np.arange(len(labels))\n",
        "    plt.bar(index*2.0,values,width = 1)\n",
        "    plt.xticks(index*2.0, labels, fontsize=15, rotation=rot)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "train_df = read_dataset('PageRank_training.csv')\n",
        "test_df = read_dataset('PageRank_test.csv')\n",
        "\n",
        "train_x, train_y = get_data_labels(train_df)\n",
        "test_x, test_y = get_data_labels(test_df)\n",
        "\n",
        "rfc = RandomForestRegressor(n_estimators=13)\n",
        "rfc.fit(train_x, train_y)\n",
        "\n",
        "predict_y = rfc.predict(test_x)\n",
        "print(13, r2_score(test_y, predict_y))\n",
        "\n",
        "'''Result: r-2 score:\n",
        "1 0.4704375446198692\n",
        "2 0.3820598185056555\n",
        "3 0.41813165684502274\n",
        "4 0.5414287643709229\n",
        "5 0.4644882858327034\n",
        "6 0.524562190075268\n",
        "7 0.4888278211071141\n",
        "8 0.5433705377008256\n",
        "9 0.5292811031839064\n",
        "10 0.4939726838345452\n",
        "11 0.5484911795022265\n",
        "12 0.5636376016628448\n",
        "\n",
        "##13 0.5730521955590606\n",
        "\n",
        "14 0.5298866281618793\n",
        "15 0.5277608069301813\n",
        "16 0.5211209258467941\n",
        "17 0.5437063733112284\n",
        "18 0.5621935228886443\n",
        "19 0.5604441107255034'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13 0.5235805346786274\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Result: r-2 score:\\n1 0.4704375446198692\\n2 0.3820598185056555\\n3 0.41813165684502274\\n4 0.5414287643709229\\n5 0.4644882858327034\\n6 0.524562190075268\\n7 0.4888278211071141\\n8 0.5433705377008256\\n9 0.5292811031839064\\n10 0.4939726838345452\\n11 0.5484911795022265\\n12 0.5636376016628448\\n\\n##13 0.5730521955590606\\n\\n14 0.5298866281618793\\n15 0.5277608069301813\\n16 0.5211209258467941\\n17 0.5437063733112284\\n18 0.5621935228886443\\n19 0.5604441107255034'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouI3m8ffhDOz",
        "colab_type": "code",
        "outputId": "7cc6057e-a5b1-497e-c6cc-a282992d4a85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.externals import joblib\n",
        "# Output a pickle file for the model\n",
        "joblib.dump(rfc, 'rfc_PageRank.pkl') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rfc_PageRank.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uE9ZsSnUQFR",
        "colab_type": "code",
        "outputId": "6e09b359-c18f-4271-ef2c-e47b39246382",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "#SGD: Random Forest \n",
        "#All imports declarations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import csv\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "np.random.seed(1)     \n",
        "\n",
        "\n",
        "def add_context(data_df, context = 0):\n",
        "\n",
        "    L = len(data_df)\n",
        "    cols = list(data_df.columns)\n",
        "\n",
        "    context_data = []\n",
        "    \n",
        "    row_1 = np.asarray(data_df.iloc[0,:])\n",
        "    row_1 = np.reshape(row_1,(1,-1))\n",
        "    row_1 = pd.DataFrame(row_1, columns=cols)\n",
        "    #pad data with data[0] context times\n",
        "    data = pd.concat([row_1]*context + [data_df])\n",
        "    \n",
        "    try:\n",
        "        del data[0]\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    data = data.to_numpy()\n",
        "    \n",
        "    \n",
        "    for fi in range(context, L):\n",
        "        frame = data[fi-context:fi+1].flatten()\n",
        "        context_data.append(frame)\n",
        "\n",
        "    return pd.DataFrame(context_data, columns = cols*(context+1))\n",
        "\n",
        "def read_dataset(filename):\n",
        "    dataset = pd.read_csv(filename)\n",
        "    df_cols = list(dataset.columns)\n",
        "    dataset = scaler.fit_transform(dataset)\n",
        "    dataset = pd.DataFrame(dataset, columns = df_cols)\n",
        "    return dataset\n",
        "\n",
        "def get_data_labels(df, context = 0):\n",
        "    label = df['flow_size'][:-context]\n",
        "    try:\n",
        "      del df['job']\n",
        "    except:\n",
        "      pass\n",
        "    df = add_context(df, context)\n",
        "    df.iloc[:, -1] = 0\n",
        "    #add context to labels\n",
        "    return df, label\n",
        "\n",
        "\n",
        "def plot_features(labels,values,title = 'Graph',rot = 90):\n",
        "    index = np.arange(len(labels))\n",
        "    plt.bar(index*2.0,values,width = 1)\n",
        "    plt.xticks(index*2.0, labels, fontsize=15, rotation=rot)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "train_df = read_dataset('SGD_training.csv')\n",
        "test_df = read_dataset('SGD_test.csv')\n",
        "\n",
        "train_x, train_y = get_data_labels(train_df)\n",
        "test_x, test_y = get_data_labels(test_df)\n",
        "\n",
        "\n",
        "rfc = RandomForestRegressor(n_estimators=18)\n",
        "rfc.fit(train_x, train_y)\n",
        "\n",
        "predict_y = rfc.predict(test_x)\n",
        "print(18, r2_score(test_y, predict_y))\n",
        "\n",
        "'''Result:\n",
        "\n",
        "1 -0.36676618487639057\n",
        "2 -1.4823518134450282\n",
        "3 0.21998995913923836\n",
        "4 -0.17838767567143532\n",
        "5 -0.23737412209230024\n",
        "6 -0.04854332631768221\n",
        "7 0.02913430209427903\n",
        "8 0.1554174558183149\n",
        "9 0.15859563247817143\n",
        "10 0.21691271463144968\n",
        "11 0.16669242552569996\n",
        "12 0.21997462907886245\n",
        "13 0.19148461580956022\n",
        "14 0.15222661084625422\n",
        "15 0.19876303291346586\n",
        "16 0.17476840341336686\n",
        "17 0.24117570733118698\n",
        "\n",
        "## 18 0.2580988072960879\n",
        "\n",
        "19 0.2579568503538321'''\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-21cb80ccfe79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mrfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mrfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mpredict_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    381\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     n_samples_bootstrap=n_samples_bootstrap)\n\u001b[0;32m--> 383\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    365\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAlluY3fhZFw",
        "colab_type": "code",
        "outputId": "7a71a966-b2a7-46e3-e9a9-5c80639a53db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.externals import joblib\n",
        "# Output a pickle file for the model\n",
        "joblib.dump(rfc, 'rfc_SGD.pkl') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rfc_SGD.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lauoMj20UQFg",
        "colab_type": "code",
        "outputId": "c143c755-0396-4798-cdff-c9ebf8aa4f7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#GradientBoosted Decision tree: KMeans:\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "np.random.seed(1)     \n",
        "\n",
        "\n",
        "def add_context(data_df, context = 0):\n",
        "\n",
        "    L = len(data_df)\n",
        "    cols = list(data_df.columns)\n",
        "\n",
        "    context_data = []\n",
        "    \n",
        "    row_1 = np.asarray(data_df.iloc[0,:])\n",
        "    row_1 = np.reshape(row_1,(1,-1))\n",
        "    row_1 = pd.DataFrame(row_1, columns=cols)\n",
        "    #pad data with data[0] context times\n",
        "    data = pd.concat([row_1]*context + [data_df])\n",
        "    \n",
        "    try:\n",
        "        del data[0]\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    data = data.to_numpy()\n",
        "    \n",
        "    \n",
        "    for fi in range(context, L):\n",
        "        frame = data[fi-context:fi+1].flatten()\n",
        "        context_data.append(frame)\n",
        "\n",
        "    return pd.DataFrame(context_data, columns = cols*(context+1))\n",
        "\n",
        "def read_dataset(filename):\n",
        "    dataset = pd.read_csv(filename)\n",
        "    df_cols = list(dataset.columns)\n",
        "    dataset = scaler.fit_transform(dataset)\n",
        "    dataset = pd.DataFrame(dataset, columns = df_cols)\n",
        "    return dataset\n",
        "\n",
        "def get_data_labels(df, context = 0):\n",
        "    label = df['flow_size'][:-context]\n",
        "    try:\n",
        "      del df['job']\n",
        "    except:\n",
        "      pass\n",
        "    df = add_context(df, context)\n",
        "    df.iloc[:, -1] = 0\n",
        "    #add context to labels\n",
        "    return df, label\n",
        "\n",
        "\n",
        "regItem = GradientBoostingRegressor( max_depth=10, n_estimators=50, learning_rate=1.0 )\n",
        "regItem.fit(training_x, training_y)\n",
        "list_of_err=[]\n",
        "\n",
        "for predicted_y in regItem.staged_predict(testing_x):\n",
        "    list_of_err.append(mean_squared_error(testing_y, predicted_y))\n",
        "topEst = np.argmin(list_of_err)\n",
        "\n",
        "bestRegItem = GradientBoostingRegressor( max_depth=7, n_estimators=topEst+1, learning_rate=1.0 )\n",
        "bestRegItem.fit(training_x, training_y)\n",
        "#predicted_score = bestRegItem.score(testing_x,testing_y)\n",
        "#predicted_y = bestRegItem.predict(testing_x)\n",
        "\n",
        "predicted_score = bestRegItem.score(testing_x,testing_y)\n",
        "predicted_y = bestRegItem.predict(testing_x)\n",
        "\n",
        "print(\"Accuracy is \",predicted_score)\n",
        "print(\"R2 is \", r2_score(testing_y, predicted_y))\n",
        "\n",
        "#Reference :https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is  0.7950709667179341\n",
            "R2 is  0.7950709667179341\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2xUWDq_h-PG",
        "colab_type": "code",
        "outputId": "4be19922-4027-430f-e606-ec8cd27b683b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.externals import joblib\n",
        "# Output a pickle file for the model\n",
        "joblib.dump(bestRegItem, 'GBDT_KMeans.pkl') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['GBDT_KMeans.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzfqYMrBZZqe",
        "colab_type": "code",
        "outputId": "3f32e06e-ad02-478b-9890-c4bbf725f358",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#GradientBoosted Decision tree: PageRank:\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "np.random.seed(1)     \n",
        "\n",
        "\n",
        "def add_context(data_df, context = 0):\n",
        "\n",
        "    L = len(data_df)\n",
        "    cols = list(data_df.columns)\n",
        "\n",
        "    context_data = []\n",
        "    \n",
        "    row_1 = np.asarray(data_df.iloc[0,:])\n",
        "    row_1 = np.reshape(row_1,(1,-1))\n",
        "    row_1 = pd.DataFrame(row_1, columns=cols)\n",
        "    #pad data with data[0] context times\n",
        "    data = pd.concat([row_1]*context + [data_df])\n",
        "    \n",
        "    try:\n",
        "        del data[0]\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    data = data.to_numpy()\n",
        "    \n",
        "    \n",
        "    for fi in range(context, len(data)):\n",
        "        frame = data[fi-context:fi+1].flatten()\n",
        "        context_data.append(frame)\n",
        "\n",
        "    return pd.DataFrame(context_data, columns = cols*(context+1))\n",
        "\n",
        "def read_dataset(filename):\n",
        "    dataset = pd.read_csv(filename)\n",
        "    df_cols = list(dataset.columns)\n",
        "    #dataset = scaler.fit_transform(dataset)\n",
        "    dataset = pd.DataFrame(dataset, columns = df_cols)\n",
        "    return dataset\n",
        "\n",
        "def get_data_labels(df, context = 0):\n",
        "    label = df['flow_size']\n",
        "    try:\n",
        "      del df['job']\n",
        "    except:\n",
        "      pass\n",
        "    df = add_context(df, context)\n",
        "    df.iloc[:, -1] = 0\n",
        "    #add context to labels\n",
        "    return df, label\n",
        "\n",
        "\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "train_df = read_dataset('PageRank_training.csv')\n",
        "test_df = read_dataset('PageRank_test.csv')\n",
        "\n",
        "context = 4\n",
        "\n",
        "training_x, training_y = get_data_labels(train_df, context)\n",
        "testing_x, testing_y = get_data_labels(test_df, context)\n",
        "\n",
        "\n",
        "regItem = GradientBoostingRegressor( max_depth=10, n_estimators=50, learning_rate=1.0 )\n",
        "regItem.fit(training_x, training_y)\n",
        "list_of_err=[]\n",
        "\n",
        "for predicted_y in regItem.staged_predict(testing_x):\n",
        "    list_of_err.append(mean_squared_error(testing_y, predicted_y))\n",
        "topEst = np.argmin(list_of_err)\n",
        "\n",
        "bestRegItem = GradientBoostingRegressor( max_depth=7, n_estimators=topEst+1, learning_rate=1.0 )\n",
        "bestRegItem.fit(training_x, training_y)\n",
        "#predicted_score = bestRegItem.score(testing_x,testing_y)\n",
        "#predicted_y = bestRegItem.predict(testing_x)\n",
        "\n",
        "predicted_score = bestRegItem.score(testing_x,testing_y)\n",
        "predicted_y = bestRegItem.predict(testing_x)\n",
        "\n",
        "print(\"Accuracy is \",predicted_score)\n",
        "print(\"R2 is \", r2_score(testing_y, predicted_y))\n",
        "\n",
        "#Reference :https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is  0.4929965962959909\n",
            "R2 is  0.4929965962959909\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkojU2R-iKxz",
        "colab_type": "code",
        "outputId": "1d98fb8a-66c3-4788-e810-063ae24b700d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.externals import joblib\n",
        "# Output a pickle file for the model\n",
        "joblib.dump(bestRegItem, 'GBDT_PageRank.pkl') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['GBDT_PageRank.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdVaAGT1ZhmI",
        "colab_type": "code",
        "outputId": "d3892a2e-478c-4b8e-da7b-40983efd3d82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#GradientBoosted Decision tree: SGD:\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "np.random.seed(1)     \n",
        "\n",
        "\n",
        "def add_context(data_df, context = 0):\n",
        "\n",
        "    L = len(data_df)\n",
        "    cols = list(data_df.columns)\n",
        "\n",
        "    context_data = []\n",
        "    \n",
        "    row_1 = np.asarray(data_df.iloc[0,:])\n",
        "    row_1 = np.reshape(row_1,(1,-1))\n",
        "    row_1 = pd.DataFrame(row_1, columns=cols)\n",
        "    #pad data with data[0] context times\n",
        "    data = pd.concat([row_1]*context + [data_df])\n",
        "    \n",
        "    try:\n",
        "        del data[0]\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    data = data.to_numpy()\n",
        "    \n",
        "    \n",
        "    for fi in range(context, L):\n",
        "        frame = data[fi-context:fi+1].flatten()\n",
        "        context_data.append(frame)\n",
        "\n",
        "    return pd.DataFrame(context_data, columns = cols*(context+1))\n",
        "\n",
        "def read_dataset(filename):\n",
        "    dataset = pd.read_csv(filename)\n",
        "    df_cols = list(dataset.columns)\n",
        "    dataset = scaler.fit_transform(dataset)\n",
        "    dataset = pd.DataFrame(dataset, columns = df_cols)\n",
        "    return dataset\n",
        "\n",
        "def get_data_labels(df, context = 0):\n",
        "    label = df['flow_size'][:-context]\n",
        "    try:\n",
        "      del df['job']\n",
        "    except:\n",
        "      pass\n",
        "    df = add_context(df, context)\n",
        "    df.iloc[:, -1] = 0\n",
        "    #add context to labels\n",
        "    return df, label\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "train_df = read_dataset('SGD_training.csv')\n",
        "test_df = read_dataset('SGD_test.csv')\n",
        "\n",
        "context = 1\n",
        "\n",
        "training_x, training_y = get_data_labels(train_df, context)\n",
        "testing_x, testing_y = get_data_labels(test_df, context)\n",
        "\n",
        "regItem = GradientBoostingRegressor( max_depth=10, n_estimators=50, learning_rate=1.0 )\n",
        "regItem.fit(training_x, training_y)\n",
        "list_of_err=[]\n",
        "\n",
        "for predicted_y in regItem.staged_predict(testing_x):\n",
        "    list_of_err.append(mean_squared_error(testing_y, predicted_y))\n",
        "topEst = np.argmin(list_of_err)\n",
        "\n",
        "bestRegItem = GradientBoostingRegressor( max_depth=7, n_estimators=topEst+1, learning_rate=1.0 )\n",
        "bestRegItem.fit(training_x, training_y)\n",
        "#predicted_score = bestRegItem.score(testing_x,testing_y)\n",
        "#predicted_y = bestRegItem.predict(testing_x)\n",
        "\n",
        "predicted_score = bestRegItem.score(testing_x,testing_y)\n",
        "predicted_y = bestRegItem.predict(testing_x)\n",
        "\n",
        "print(\"Accuracy is \",predicted_score)\n",
        "print(\"R2 is \", r2_score(testing_y, predicted_y))\n",
        "\n",
        "#Reference :https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is  0.2026717188914322\n",
            "R2 is  0.2026717188914322\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id7tagTqiQL5",
        "colab_type": "code",
        "outputId": "b5116271-2ce6-4824-d103-8760341eded8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.externals import joblib\n",
        "# Output a pickle file for the model\n",
        "joblib.dump(bestRegItem, 'GBDT_SGD.pkl') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['GBDT_SGD.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6A7CIYGjiXw",
        "colab_type": "code",
        "outputId": "7a582905-03d7-4914-9eb2-71cccdc4a5cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "#GradientBoosted Decision tree: Full data:\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "def read_dataset(filename):\n",
        "    dataset = pd.read_csv(filename)#training_set_with_cue.csv\n",
        "    return dataset\n",
        "def get_data_labels(df):\n",
        "    label = df['flow_size']\n",
        "    del df['flow_size']\n",
        "    return df, label\n",
        "training_df = read_dataset('training3.csv')\n",
        "test_df = read_dataset('test3.csv')\n",
        "training_x, training_y = get_data_labels(training_df)\n",
        "testing_x, testing_y = get_data_labels(test_df)\n",
        "\n",
        "\n",
        "regItem = GradientBoostingRegressor( max_depth=10, n_estimators=50, learning_rate=1.0 )\n",
        "regItem.fit(training_x, training_y)\n",
        "list_of_err=[]\n",
        "\n",
        "for predicted_y in regItem.staged_predict(testing_x):\n",
        "    list_of_err.append(mean_squared_error(testing_y, predicted_y))\n",
        "topEst = np.argmin(list_of_err)\n",
        "\n",
        "bestRegItem = GradientBoostingRegressor( max_depth=7, n_estimators=topEst+1, learning_rate=1.0 )\n",
        "bestRegItem.fit(training_x, training_y)\n",
        "#predicted_score = bestRegItem.score(testing_x,testing_y)\n",
        "#predicted_y = bestRegItem.predict(testing_x)\n",
        "\n",
        "predicted_score = bestRegItem.score(testing_x,testing_y)\n",
        "predicted_y = bestRegItem.predict(testing_x)\n",
        "\n",
        "print(\"Accuracy is \",predicted_score)\n",
        "print(\"R2 is \", r2_score(testing_y, predicted_y))\n",
        "\n",
        "#Reference :https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-378e695ddd01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mregItem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientBoostingRegressor\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mregItem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mlist_of_err\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1439\u001b[0m         \u001b[0;31m# Since check_array converts both X and y to the same dtype, but the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;31m# trees use different types for X and y, checking them separately.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1442\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 578\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IO1LXdzj4ih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.externals import joblib\n",
        "# Output a pickle file for the model\n",
        "joblib.dump(bestRegItem, 'GBDT_3.pkl') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbKjSruIj9VT",
        "colab_type": "code",
        "outputId": "a99ac49c-1d5d-4258-c210-e89c56c402ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "#SGD: Random Forest \n",
        "#All imports declarations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import csv\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def read_dataset(filename):\n",
        "    dataset = pd.read_csv(filename)#training_set_with_cue.csv\n",
        "    return dataset\n",
        "def get_data_labels(df):\n",
        "    label = df['flow_size']\n",
        "    del df['flow_size']\n",
        "    return df, label\n",
        "\n",
        "\n",
        "def plot_features(labels,values,title = 'Graph',rot = 90):\n",
        "    index = np.arange(len(labels))\n",
        "    plt.bar(index*2.0,values,width = 1)\n",
        "    plt.xticks(index*2.0, labels, fontsize=15, rotation=rot)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "train_df = read_dataset('training3.csv')\n",
        "test_df = read_dataset('test3.csv')\n",
        "\n",
        "train_x, train_y = get_data_labels(train_df)\n",
        "test_x, test_y = get_data_labels(test_df)\n",
        "\n",
        "for i in range(1,20):\n",
        "    rfc = RandomForestRegressor(n_estimators=i)\n",
        "    rfc.fit(train_x, train_y)\n",
        "\n",
        "    predict_y = rfc.predict(test_x)\n",
        "    print(i, r2_score(test_y, predict_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-401255689cb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mrfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mrfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mpredict_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \"\"\"\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 578\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISsM8PMQj9Fe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.externals import joblib\n",
        "# Output a pickle file for the model\n",
        "joblib.dump(rfc, 'rfc_3.pkl') "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}