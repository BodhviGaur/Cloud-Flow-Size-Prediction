{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Random Forest And GBDT with Context 1",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "inPHt6oQbEKh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "c7e846c5-18dd-406f-e3b0-233b86301dbe"
      },
      "source": [
        "# SGD- Random Forest:\n",
        "\n",
        "#All imports declarations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "np.random.seed(1)     \n",
        "\n",
        "def add_context(data_df, context = 0):\n",
        "    L = len(data_df)\n",
        "    cols = list(data_df.columns)\n",
        "    context_data = []\n",
        "    row_1 = np.asarray(data_df.iloc[0,:])\n",
        "    row_1 = np.reshape(row_1,(1,-1))\n",
        "    row_1 = pd.DataFrame(row_1, columns=cols)\n",
        "    #pad data with data[0] context times\n",
        "    data = pd.concat([row_1]*context + [data_df])\n",
        "    try:\n",
        "        del data[0]\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    data = data.to_numpy()\n",
        "    for fi in range(context, len(data)):\n",
        "        frame = data[fi-context:fi+1].flatten()\n",
        "        context_data.append(frame)\n",
        "    return pd.DataFrame(context_data, columns = cols*(context+1))\n",
        "\n",
        "'''def read_dataset(filename):\n",
        "    dataset = pd.read_csv(filename)\n",
        "    df_cols = list(dataset.columns)\n",
        "    dataset = scaler.fit_transform(dataset)\n",
        "    dataset = pd.DataFrame(dataset, columns = df_cols)\n",
        "    return dataset'''\n",
        "\n",
        "'''\n",
        "updated read_dataset Function\n",
        "'''    \n",
        "def read_dataset(filename):\n",
        "    dataset = pd.read_csv(filename)\n",
        "    return dataset\n",
        "\n",
        "def get_data_labels(df, context = 0):\n",
        "    label = df['flow_size']\n",
        "    #label = df['flow_size']\n",
        "    try:\n",
        "      del df['job']\n",
        "    except:\n",
        "      pass\n",
        "    df = add_context(df, context)\n",
        "    df.iloc[:, -1] = 0\n",
        "    #add context to labels\n",
        "    return df, label\n",
        "\n",
        "\n",
        "def plot_features(labels,values,title = 'Graph',rot = 90):\n",
        "    index = np.arange(len(labels))\n",
        "    plt.bar(index*2.0,values,width = 1)\n",
        "    plt.xticks(index*2.0, labels, fontsize=15, rotation=rot)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "'''\n",
        "Updated train, test values '''\n",
        "#need to scale values to the range(0,1)\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "\n",
        "train_df = read_dataset('SGD_training.csv')\n",
        "test_df = read_dataset('SGD_test.csv')\n",
        "validation_df= read_dataset('SGD_validation.csv')\n",
        "\n",
        "#scale and convert to df\n",
        "#fit on train, transform train, test, validation\n",
        "cols = list(train_df.columns)\n",
        "train_df = scaler.fit_transform(train_df)\n",
        "train_df = pd.DataFrame(train_df, columns = cols)\n",
        "\n",
        "test_df = scaler.transform(test_df)\n",
        "test_df = pd.DataFrame(test_df, columns = cols)\n",
        "\n",
        "validation_df = scaler.transform(validation_df)\n",
        "validation_df = pd.DataFrame(validation_df, columns = cols)\n",
        "\n",
        "context = 1\n",
        "\n",
        "train_x, train_y = get_data_labels(train_df, context)\n",
        "test_x, test_y = get_data_labels(test_df, context)\n",
        "validation_x, validation_y = get_data_labels(validation_df, context)\n",
        "\n",
        "'''for i in range(1,20):\n",
        "  rfc = RandomForestRegressor(n_estimators=i)\n",
        "  rfc.fit(train_x, train_y)\n",
        "  predict_y = rfc.predict(test_x)\n",
        "  print(\"size testdf:\", len(test_df))\n",
        "  print(\"size traindf:\", len(train_df))\n",
        "  print(\"size test_x:\", len(test_x))\n",
        "  print(\"size train_x:\", len(train_x))\n",
        "  print(i, r2_score(test_y, predict_y))'''\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 0.1677480204560602\n",
            "2 -0.10969860722330771\n",
            "3 0.058444425368974984\n",
            "4 0.27632827475067234\n",
            "5 0.13125661712897685\n",
            "6 0.1725854816541278\n",
            "7 0.21841283316938298\n",
            "8 0.3312008362389025\n",
            "9 0.388524756909286\n",
            "10 0.3154979811279093\n",
            "11 0.1761524922299651\n",
            "12 0.222047643597877\n",
            "13 0.346536198505659\n",
            "14 0.31892956594922084\n",
            "15 0.16873609997574246\n",
            "16 0.09460996621746864\n",
            "17 0.2902495819247325\n",
            "18 0.42163978297815685\n",
            "19 0.30777756429841296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_Tbblc9DGgc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ba392edf-c436-4ae2-e383-2844b6aec463"
      },
      "source": [
        "rfc = RandomForestRegressor(n_estimators=19)\n",
        "rfc.fit(train_x, train_y)\n",
        "predict_y = rfc.predict(test_x)\n",
        "print(19, r2_score(test_y, predict_y))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19 0.3823405880412115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38ir2s8SgwiA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "c2bfa782-e051-472a-c815-09ebf9d6621a"
      },
      "source": [
        "test_mini_df = read_dataset('SGD_test_jb_mini.csv')\n",
        "#jobs = test_mini_df['job']\n",
        "#del test_mini_df['job']\n",
        "print('predict_y:', len(predict_y))\n",
        "print('test_df',test_df)\n",
        "#get original test_df\n",
        "test_df['flow_size'] = predict_y\n",
        "#print(test_df.tail())\n",
        "unscaled_test_df = scaler.inverse_transform(test_df)\n",
        "unscaled_test_df = pd.DataFrame(unscaled_test_df, columns = cols)\n",
        "#change_cols = {\"agg_net_out\":'int32', \"agg_net_in\":'int32', \"agg_net_out_per_machine\":'int32','agg_net_in_per_machine':'int32','machine':'int32','flow_size':'int32'}\n",
        "#unscaled_test_df = unscaled_test_df.astype(change_cols)\n",
        "unscaled_test_df.tail(20)\n",
        "\n",
        "new_df = pd.DataFrame([])\n",
        "new_df['flow_size'] = test_mini_df['flow_size']\n",
        "new_df['job'] = test_mini_df['job']\n",
        "new_df['predicted_flow_size'] = test_mini_df['flow_size']\n",
        "\n",
        "#now copy predicted\n",
        "for index, row in test_mini_df.iterrows():\n",
        "  i = int(row['pseudo_index'])\n",
        "  new_df.iloc[index,2] = unscaled_test_df.iloc[i]['flow_size']\n",
        "\n",
        "new_df.to_csv('SGD_test_jb_mini_predicted_no_outliers.csv')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict_y: 57531\n",
            "test_df            time  first_call  machine  ...  agg_net_in  flow_frequency  flow_size\n",
            "0      0.999448    0.000919  0.00000  ...    0.000001    0.000000e+00   0.000002\n",
            "1      0.999448    0.000689  0.00000  ...    0.000002    6.626595e-09   0.000001\n",
            "2      0.999448    0.000689  0.00000  ...    0.000002    1.744291e-07   0.000001\n",
            "3      0.999448    0.000689  0.00000  ...    0.000003    1.728982e-07   0.000001\n",
            "4      0.999448    0.000689  0.00000  ...    0.000004    1.728667e-07   0.000001\n",
            "...         ...         ...      ...  ...         ...             ...        ...\n",
            "57526  0.999617    0.000832  0.00000  ...    0.796774    0.000000e+00   0.000001\n",
            "57527  0.999617    0.000716  0.00641  ...    0.796774    4.252106e-08   0.000001\n",
            "57528  0.999617    0.000832  0.00000  ...    0.796774    1.302607e-07   0.000001\n",
            "57529  0.999618    0.000832  0.00000  ...    0.796775    1.728106e-07   0.000001\n",
            "57530  0.999618    0.000832  0.00000  ...    0.796778    1.727876e-07   0.000001\n",
            "\n",
            "[57531 rows x 9 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnHjYiQxEBoJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "4371a655-bb29-45d0-d589-4172c1c971e7"
      },
      "source": [
        "# KMeans- Random Forest:\n",
        "\n",
        "#All imports declarations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "np.random.seed(1)     \n",
        "\n",
        "def add_context(data_df, context = 0):\n",
        "    L = len(data_df)\n",
        "    cols = list(data_df.columns)\n",
        "    context_data = []\n",
        "    row_1 = np.asarray(data_df.iloc[0,:])\n",
        "    row_1 = np.reshape(row_1,(1,-1))\n",
        "    row_1 = pd.DataFrame(row_1, columns=cols)\n",
        "    #pad data with data[0] context times\n",
        "    data = pd.concat([row_1]*context + [data_df])\n",
        "    try:\n",
        "        del data[0]\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    data = data.to_numpy()\n",
        "    for fi in range(context, len(data)):\n",
        "        frame = data[fi-context:fi+1].flatten()\n",
        "        context_data.append(frame)\n",
        "    return pd.DataFrame(context_data, columns = cols*(context+1))\n",
        "\n",
        "'''def read_dataset(filename):\n",
        "    dataset = pd.read_csv(filename)\n",
        "    df_cols = list(dataset.columns)\n",
        "    dataset = scaler.fit_transform(dataset)\n",
        "    dataset = pd.DataFrame(dataset, columns = df_cols)\n",
        "    return dataset'''\n",
        "\n",
        "'''\n",
        "updated read_dataset Function\n",
        "'''    \n",
        "def read_dataset(filename):\n",
        "    dataset = pd.read_csv(filename)\n",
        "    return dataset\n",
        "\n",
        "def get_data_labels(df, context = 0):\n",
        "    label = df['flow_size']\n",
        "    #label = df['flow_size']\n",
        "    try:\n",
        "      del df['job']\n",
        "    except:\n",
        "      pass\n",
        "    df = add_context(df, context)\n",
        "    df.iloc[:, -1] = 0\n",
        "    #add context to labels\n",
        "    return df, label\n",
        "\n",
        "\n",
        "def plot_features(labels,values,title = 'Graph',rot = 90):\n",
        "    index = np.arange(len(labels))\n",
        "    plt.bar(index*2.0,values,width = 1)\n",
        "    plt.xticks(index*2.0, labels, fontsize=15, rotation=rot)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "'''\n",
        "Updated train, test values '''\n",
        "#need to scale values to the range(0,1)\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "\n",
        "train_df = read_dataset('KMeans_training.csv')\n",
        "test_df = read_dataset('KMeans_test.csv')\n",
        "validation_df= read_dataset('KMeans_validation.csv')\n",
        "\n",
        "#scale and convert to df\n",
        "#fit on train, transform train, test, validation\n",
        "cols = list(train_df.columns)\n",
        "train_df = scaler.fit_transform(train_df)\n",
        "train_df = pd.DataFrame(train_df, columns = cols)\n",
        "\n",
        "test_df = scaler.transform(test_df)\n",
        "test_df = pd.DataFrame(test_df, columns = cols)\n",
        "\n",
        "validation_df = scaler.transform(validation_df)\n",
        "validation_df = pd.DataFrame(validation_df, columns = cols)\n",
        "\n",
        "context = 1\n",
        "\n",
        "train_x, train_y = get_data_labels(train_df, context)\n",
        "test_x, test_y = get_data_labels(test_df, context)\n",
        "validation_x, validation_y = get_data_labels(validation_df, context)\n",
        "\n",
        "for i in range(1,20):\n",
        "  rfc = RandomForestRegressor(n_estimators=i)\n",
        "  rfc.fit(train_x, train_y)\n",
        "  predict_y = rfc.predict(test_x)\n",
        "  #print(\"size testdf:\", len(test_df))\n",
        "  #print(\"size traindf:\", len(train_df))\n",
        "  #print(\"size test_x:\", len(test_x))\n",
        "  #print(\"size train_x:\", len(train_x))\n",
        "  print(i, r2_score(test_y, predict_y))\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 0.8193058378072318\n",
            "2 0.893250688327546\n",
            "3 0.8484804431476178\n",
            "4 0.8690003529666837\n",
            "5 0.8789231554427571\n",
            "6 0.8946355322412317\n",
            "7 0.8745326929628829\n",
            "8 0.8909622797031816\n",
            "9 0.8958775948556043\n",
            "10 0.899345826667517\n",
            "11 0.8984087445904212\n",
            "12 0.8958658072145758\n",
            "13 0.8908794054592907\n",
            "14 0.8803767426835489\n",
            "15 0.8973523046772676\n",
            "16 0.8980961748829122\n",
            "17 0.8931272365177727\n",
            "18 0.9050718577854955\n",
            "19 0.9059467778204662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh292GhrEB4H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "561df745-c4be-498c-d02c-274372b83a09"
      },
      "source": [
        "rfc = RandomForestRegressor(n_estimators=19)\n",
        "rfc.fit(train_x, train_y)\n",
        "predict_y = rfc.predict(test_x)\n",
        "print(19, r2_score(test_y, predict_y))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19 0.898272731062778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77BJ3NDZECA2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "3193df03-54fe-462c-de93-476202d87d9a"
      },
      "source": [
        "test_mini_df = read_dataset('Kmeans_test_jb_mini.csv')\n",
        "#jobs = test_mini_df['job']\n",
        "#del test_mini_df['job']\n",
        "print('predict_y:', len(predict_y))\n",
        "print('test_df',test_df)\n",
        "#get original test_df\n",
        "test_df['flow_size'] = predict_y\n",
        "#print(test_df.tail())\n",
        "unscaled_test_df = scaler.inverse_transform(test_df)\n",
        "unscaled_test_df = pd.DataFrame(unscaled_test_df, columns = cols)\n",
        "#change_cols = {\"agg_net_out\":'int32', \"agg_net_in\":'int32', \"agg_net_out_per_machine\":'int32','agg_net_in_per_machine':'int32','machine':'int32','flow_size':'int32'}\n",
        "#unscaled_test_df = unscaled_test_df.astype(change_cols)\n",
        "unscaled_test_df.tail(20)\n",
        "\n",
        "new_df = pd.DataFrame([])\n",
        "new_df['flow_size'] = test_mini_df['flow_size']\n",
        "new_df['job'] = test_mini_df['job']\n",
        "new_df['predicted_flow_size'] = test_mini_df['flow_size']\n",
        "\n",
        "#now copy predicted\n",
        "for index, row in test_mini_df.iterrows():\n",
        "  i = int(row['pseudo_index'])\n",
        "  new_df.iloc[index,2] = unscaled_test_df.iloc[i]['flow_size']\n",
        "\n",
        "new_df.to_csv('KMeans_test_jb_mini_predicted_no_outliers.csv')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict_y: 15730\n",
            "test_df            time  previous_flow_time  ...  first_call     flow_size\n",
            "0      0.060262            0.040180  ...    0.015248  2.508981e-06\n",
            "1      0.078166            0.019965  ...    0.026744  4.400595e-06\n",
            "2      0.080349            0.001248  ...    0.031606  5.200703e-06\n",
            "3      0.082314            0.001497  ...    0.015528  2.555078e-06\n",
            "4      0.084061            0.001497  ...    0.014428  2.373983e-06\n",
            "...         ...                 ...  ...         ...           ...\n",
            "15725  0.837773            0.325430  ...    0.079221  1.303551e-05\n",
            "15726  0.837773            0.325430  ...    0.000520  8.560828e-08\n",
            "15727  0.837773            0.009483  ...    0.000000  0.000000e+00\n",
            "15728  0.837773            0.325680  ...    0.000000  0.000000e+00\n",
            "15729  0.838646            0.014724  ...    0.000520  8.560828e-08\n",
            "\n",
            "[15730 rows x 15 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbTrhI2aFyrL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "daabd4b6-9078-4f6e-ff0f-464e7090da9f"
      },
      "source": [
        "# PageRank- Random Forest:\n",
        "\n",
        "#All imports declarations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "np.random.seed(1)     \n",
        "\n",
        "def add_context(data_df, context = 0):\n",
        "    L = len(data_df)\n",
        "    cols = list(data_df.columns)\n",
        "    context_data = []\n",
        "    row_1 = np.asarray(data_df.iloc[0,:])\n",
        "    row_1 = np.reshape(row_1,(1,-1))\n",
        "    row_1 = pd.DataFrame(row_1, columns=cols)\n",
        "    #pad data with data[0] context times\n",
        "    data = pd.concat([row_1]*context + [data_df])\n",
        "    try:\n",
        "        del data[0]\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    data = data.to_numpy()\n",
        "    for fi in range(context, len(data)):\n",
        "        frame = data[fi-context:fi+1].flatten()\n",
        "        context_data.append(frame)\n",
        "    return pd.DataFrame(context_data, columns = cols*(context+1))\n",
        "\n",
        "'''def read_dataset(filename):\n",
        "    dataset = pd.read_csv(filename)\n",
        "    df_cols = list(dataset.columns)\n",
        "    dataset = scaler.fit_transform(dataset)\n",
        "    dataset = pd.DataFrame(dataset, columns = df_cols)\n",
        "    return dataset'''\n",
        "\n",
        "'''\n",
        "updated read_dataset Function\n",
        "'''    \n",
        "def read_dataset(filename):\n",
        "    dataset = pd.read_csv(filename)\n",
        "    return dataset\n",
        "\n",
        "def get_data_labels(df, context = 0):\n",
        "    label = df['flow_size']\n",
        "    #label = df['flow_size']\n",
        "    try:\n",
        "      del df['job']\n",
        "    except:\n",
        "      pass\n",
        "    df = add_context(df, context)\n",
        "    df.iloc[:, -1] = 0\n",
        "    #add context to labels\n",
        "    return df, label\n",
        "\n",
        "\n",
        "def plot_features(labels,values,title = 'Graph',rot = 90):\n",
        "    index = np.arange(len(labels))\n",
        "    plt.bar(index*2.0,values,width = 1)\n",
        "    plt.xticks(index*2.0, labels, fontsize=15, rotation=rot)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "'''\n",
        "Updated train, test values '''\n",
        "#need to scale values to the range(0,1)\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "\n",
        "train_df = read_dataset('PageRank_training.csv')\n",
        "test_df = read_dataset('PageRank_test.csv')\n",
        "validation_df= read_dataset('PageRank_validation.csv')\n",
        "\n",
        "#scale and convert to df\n",
        "#fit on train, transform train, test, validation\n",
        "cols = list(train_df.columns)\n",
        "train_df = scaler.fit_transform(train_df)\n",
        "train_df = pd.DataFrame(train_df, columns = cols)\n",
        "\n",
        "test_df = scaler.transform(test_df)\n",
        "test_df = pd.DataFrame(test_df, columns = cols)\n",
        "\n",
        "validation_df = scaler.transform(validation_df)\n",
        "validation_df = pd.DataFrame(validation_df, columns = cols)\n",
        "\n",
        "context = 1\n",
        "\n",
        "train_x, train_y = get_data_labels(train_df, context)\n",
        "test_x, test_y = get_data_labels(test_df, context)\n",
        "validation_x, validation_y = get_data_labels(validation_df, context)\n",
        "\n",
        "for i in range(1,20):\n",
        "  rfc = RandomForestRegressor(n_estimators=i)\n",
        "  rfc.fit(train_x, train_y)\n",
        "  predict_y = rfc.predict(test_x)\n",
        "  #print(\"size testdf:\", len(test_df))\n",
        "  #print(\"size traindf:\", len(train_df))\n",
        "  #print(\"size test_x:\", len(test_x))\n",
        "  #print(\"size train_x:\", len(train_x))\n",
        "  print(i, r2_score(test_y, predict_y))\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 0.6983204319894609\n",
            "2 0.7101900756688395\n",
            "3 0.757167437911398\n",
            "4 0.8269124666741674\n",
            "5 0.7651784475131902\n",
            "6 0.7824328319791763\n",
            "7 0.7721773807395867\n",
            "8 0.7975466126788959\n",
            "9 0.8290804971044602\n",
            "10 0.8476087709866041\n",
            "11 0.8267697655301125\n",
            "12 0.8231675467612243\n",
            "13 0.7711488647507884\n",
            "14 0.8211419930522503\n",
            "15 0.8527223829500807\n",
            "16 0.8526860553836406\n",
            "17 0.8310418960705626\n",
            "18 0.8142875091101824\n",
            "19 0.8370081084500898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TsrljPYFyx1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39e80ef8-cf81-4e17-e466-e2ac02e69c49"
      },
      "source": [
        "rfc = RandomForestRegressor(n_estimators=15)\n",
        "rfc.fit(train_x, train_y)\n",
        "predict_y = rfc.predict(test_x)\n",
        "print(15, r2_score(test_y, predict_y))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15 0.8399089953846118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAqRFiYiFy78",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "976f01d9-4e47-4202-9852-223a7580308f"
      },
      "source": [
        "test_mini_df = read_dataset('PageRank_test_jb_mini.csv')\n",
        "#jobs = test_mini_df['job']\n",
        "#del test_mini_df['job']\n",
        "print('predict_y:', len(predict_y))\n",
        "print('test_df',test_df)\n",
        "#get original test_df\n",
        "test_df['flow_size'] = predict_y\n",
        "#print(test_df.tail())\n",
        "unscaled_test_df = scaler.inverse_transform(test_df)\n",
        "unscaled_test_df = pd.DataFrame(unscaled_test_df, columns = cols)\n",
        "#change_cols = {\"agg_net_out\":'int32', \"agg_net_in\":'int32', \"agg_net_out_per_machine\":'int32','agg_net_in_per_machine':'int32','machine':'int32','flow_size':'int32'}\n",
        "#unscaled_test_df = unscaled_test_df.astype(change_cols)\n",
        "unscaled_test_df.tail(20)\n",
        "\n",
        "new_df = pd.DataFrame([])\n",
        "new_df['flow_size'] = test_mini_df['flow_size']\n",
        "new_df['job'] = test_mini_df['job']\n",
        "new_df['predicted_flow_size'] = test_mini_df['flow_size']\n",
        "\n",
        "#now copy predicted\n",
        "for index, row in test_mini_df.iterrows():\n",
        "  i = int(row['pseudo_index'])\n",
        "  new_df.iloc[index,2] = unscaled_test_df.iloc[i]['flow_size']\n",
        "\n",
        "new_df.to_csv('PageRank_test_jb_mini_predicted_no_outliers.csv')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict_y: 14814\n",
            "test_df            time  flow_frequency  ...   machine     flow_size\n",
            "0      0.060141        0.025600  ...  0.666667  9.449852e-07\n",
            "1      0.067399        0.009333  ...  0.666667  2.922210e-06\n",
            "2      0.068644        0.000800  ...  0.666667  6.191793e-06\n",
            "3      0.069681        0.001333  ...  0.666667  8.099873e-07\n",
            "4      0.070925        0.000533  ...  0.666667  4.892027e-05\n",
            "...         ...             ...  ...       ...           ...\n",
            "14809  0.469307        0.002667  ...  0.666667  4.581697e-05\n",
            "14810  0.469929        0.000800  ...  0.666667  3.121908e-05\n",
            "14811  0.471174        0.001333  ...  0.666667  3.139030e-05\n",
            "14812  0.471796        0.204800  ...  0.000000  1.768966e-05\n",
            "14813  0.000000        0.000000  ...  0.000000  2.825078e-06\n",
            "\n",
            "[14814 rows x 8 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eihNuvQ-t5XC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "26b99774-227c-4f86-9266-12c8fd5d91f6"
      },
      "source": [
        "#GradientBoosted Decision tree: PageRank:\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "np.random.seed(1)     \n",
        "\n",
        "\n",
        "def add_context(data_df, context = 0):\n",
        "\n",
        "    L = len(data_df)\n",
        "    cols = list(data_df.columns)\n",
        "\n",
        "    context_data = []\n",
        "    \n",
        "    row_1 = np.asarray(data_df.iloc[0,:])\n",
        "    row_1 = np.reshape(row_1,(1,-1))\n",
        "    row_1 = pd.DataFrame(row_1, columns=cols)\n",
        "    #pad data with data[0] context times\n",
        "    data = pd.concat([row_1]*context + [data_df])\n",
        "    \n",
        "    try:\n",
        "        del data[0]\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    data = data.to_numpy()\n",
        "    \n",
        "    \n",
        "    for fi in range(context, len(data)):\n",
        "        frame = data[fi-context:fi+1].flatten()\n",
        "        context_data.append(frame)\n",
        "\n",
        "    return pd.DataFrame(context_data, columns = cols*(context+1))\n",
        "\n",
        "'''def read_dataset(filename):\n",
        "    dataset = pd.read_csv(filename)\n",
        "    df_cols = list(dataset.columns)\n",
        "    #dataset = scaler.fit_transform(dataset)\n",
        "    dataset = pd.DataFrame(dataset, columns = df_cols)\n",
        "    return dataset\n",
        "'''\n",
        "\n",
        "def read_dataset(filename):\n",
        "    dataset = pd.read_csv(filename)\n",
        "    return dataset\n",
        "\n",
        "def get_data_labels(df, context = 0):\n",
        "    label = df['flow_size']\n",
        "    try:\n",
        "      del df['job']\n",
        "    except:\n",
        "      pass\n",
        "    df = add_context(df, context)\n",
        "    df.iloc[:, -1] = 0\n",
        "    #add context to labels\n",
        "    return df, label\n",
        "\n",
        "\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "train_df = read_dataset('PageRank_training.csv')\n",
        "test_df = read_dataset('PageRank_test.csv')\n",
        "\n",
        "#context = 4\n",
        "context=1\n",
        "\n",
        "cols = train_df.columns\n",
        "\n",
        "#scale and convert to df\n",
        "#fit on train, transform train, test, validation\n",
        "train_df = scaler.fit_transform(train_df)\n",
        "train_df = pd.DataFrame(train_df, columns = cols)\n",
        "\n",
        "test_df = scaler.transform(test_df)\n",
        "test_df = pd.DataFrame(test_df, columns = cols)\n",
        "\n",
        "\n",
        "training_x, training_y = get_data_labels(train_df, context)\n",
        "testing_x, testing_y = get_data_labels(test_df, context)\n",
        "\n",
        "\n",
        "regItem = GradientBoostingRegressor( max_depth=10, n_estimators=50, learning_rate=1.0 )\n",
        "regItem.fit(training_x, training_y)\n",
        "list_of_err=[]\n",
        "\n",
        "for predicted_y in regItem.staged_predict(testing_x):\n",
        "    list_of_err.append(mean_squared_error(testing_y, predicted_y))\n",
        "topEst = np.argmin(list_of_err)\n",
        "\n",
        "bestRegItem = GradientBoostingRegressor( max_depth=7, n_estimators=topEst+1, learning_rate=1.0 )\n",
        "bestRegItem.fit(training_x, training_y)\n",
        "#predicted_score = bestRegItem.score(testing_x,testing_y)\n",
        "#predicted_y = bestRegItem.predict(testing_x)\n",
        "\n",
        "predicted_score = bestRegItem.score(testing_x,testing_y)\n",
        "predicted_y = bestRegItem.predict(testing_x)\n",
        "\n",
        "print(\"Accuracy is \",predicted_score)\n",
        "print(\"R2 is \", r2_score(testing_y, predicted_y))\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is  0.5986977910437798\n",
            "R2 is  0.5986977910437798\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtnnQRB_JaHQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_mini_df = read_dataset('PageRank_test_jb_mini.csv')\n",
        "#jobs = test_mini_df['job']\n",
        "#del test_mini_df['job']\n",
        "\n",
        "#get original test_df\n",
        "test_df['flow_size'] = predicted_y\n",
        "#print(test_df.tail())\n",
        "unscaled_test_df = scaler.inverse_transform(test_df)\n",
        "unscaled_test_df = pd.DataFrame(unscaled_test_df, columns = cols)\n",
        "#change_cols = {\"agg_net_out\":'int32', \"agg_net_in\":'int32', \"agg_net_out_per_machine\":'int32','agg_net_in_per_machine':'int32','machine':'int32','flow_size':'int32'}\n",
        "#unscaled_test_df = unscaled_test_df.astype(change_cols)\n",
        "unscaled_test_df.tail(20)\n",
        "\n",
        "new_df = pd.DataFrame([])\n",
        "new_df['flow_size'] = test_mini_df['flow_size']\n",
        "new_df['job'] = test_mini_df['job']\n",
        "new_df['predicted_flow_size'] = test_mini_df['flow_size']\n",
        "\n",
        "#now copy predicted\n",
        "for index, row in test_mini_df.iterrows():\n",
        "  i = int(row['pseudo_index'])\n",
        "  new_df.iloc[index,2] = unscaled_test_df.iloc[i]['flow_size']\n",
        "\n",
        "new_df.to_csv('PageRank_test_jb_mini_predicted_no_outliers_GBDT.csv')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHjUtcVxJp24",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "9e539f13-4af4-4e5f-c586-74fcec29602c"
      },
      "source": [
        "#GradientBoosted Decision tree: KMeans:\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "np.random.seed(1)     \n",
        "\n",
        "\n",
        "def add_context(data_df, context = 0):\n",
        "\n",
        "    L = len(data_df)\n",
        "    cols = list(data_df.columns)\n",
        "\n",
        "    context_data = []\n",
        "    \n",
        "    row_1 = np.asarray(data_df.iloc[0,:])\n",
        "    row_1 = np.reshape(row_1,(1,-1))\n",
        "    row_1 = pd.DataFrame(row_1, columns=cols)\n",
        "    #pad data with data[0] context times\n",
        "    data = pd.concat([row_1]*context + [data_df])\n",
        "    \n",
        "    try:\n",
        "        del data[0]\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    data = data.to_numpy()\n",
        "    \n",
        "    \n",
        "    for fi in range(context, len(data)):\n",
        "        frame = data[fi-context:fi+1].flatten()\n",
        "        context_data.append(frame)\n",
        "\n",
        "    return pd.DataFrame(context_data, columns = cols*(context+1))\n",
        "\n",
        "def read_dataset(filename):\n",
        "    dataset = pd.read_csv(filename)\n",
        "    return dataset\n",
        "\n",
        "def get_data_labels(df, context = 0):\n",
        "    label = df['flow_size']\n",
        "    try:\n",
        "      del df['job']\n",
        "    except:\n",
        "      pass\n",
        "    df = add_context(df, context)\n",
        "    df.iloc[:, -1] = 0\n",
        "    #add context to labels\n",
        "    return df, label\n",
        "\n",
        "\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "train_df = read_dataset('KMeans_training.csv')\n",
        "test_df = read_dataset('KMeans_test.csv')\n",
        "\n",
        "#context = 4\n",
        "context=1\n",
        "\n",
        "cols = train_df.columns\n",
        "\n",
        "#scale and convert to df\n",
        "#fit on train, transform train, test, validation\n",
        "train_df = scaler.fit_transform(train_df)\n",
        "train_df = pd.DataFrame(train_df, columns = cols)\n",
        "\n",
        "test_df = scaler.transform(test_df)\n",
        "test_df = pd.DataFrame(test_df, columns = cols)\n",
        "\n",
        "\n",
        "training_x, training_y = get_data_labels(train_df, context)\n",
        "testing_x, testing_y = get_data_labels(test_df, context)\n",
        "\n",
        "\n",
        "regItem = GradientBoostingRegressor( max_depth=10, n_estimators=50, learning_rate=1.0 )\n",
        "regItem.fit(training_x, training_y)\n",
        "list_of_err=[]\n",
        "\n",
        "for predicted_y in regItem.staged_predict(testing_x):\n",
        "    list_of_err.append(mean_squared_error(testing_y, predicted_y))\n",
        "topEst = np.argmin(list_of_err)\n",
        "\n",
        "bestRegItem = GradientBoostingRegressor( max_depth=7, n_estimators=topEst+1, learning_rate=1.0 )\n",
        "bestRegItem.fit(training_x, training_y)\n",
        "#predicted_score = bestRegItem.score(testing_x,testing_y)\n",
        "#predicted_y = bestRegItem.predict(testing_x)\n",
        "\n",
        "predicted_score = bestRegItem.score(testing_x,testing_y)\n",
        "predicted_y = bestRegItem.predict(testing_x)\n",
        "\n",
        "print(\"Accuracy is \",predicted_score)\n",
        "print(\"R2 is \", r2_score(testing_y, predicted_y))\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is  0.8548974600609157\n",
            "R2 is  0.8548974600609157\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb5N3z9GKBls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_mini_df = read_dataset('Kmeans_test_jb_mini.csv')\n",
        "#jobs = test_mini_df['job']\n",
        "#del test_mini_df['job']\n",
        "\n",
        "#get original test_df\n",
        "test_df['flow_size'] = predicted_y\n",
        "#print(test_df.tail())\n",
        "unscaled_test_df = scaler.inverse_transform(test_df)\n",
        "unscaled_test_df = pd.DataFrame(unscaled_test_df, columns = cols)\n",
        "#change_cols = {\"agg_net_out\":'int32', \"agg_net_in\":'int32', \"agg_net_out_per_machine\":'int32','agg_net_in_per_machine':'int32','machine':'int32','flow_size':'int32'}\n",
        "#unscaled_test_df = unscaled_test_df.astype(change_cols)\n",
        "unscaled_test_df.tail(20)\n",
        "\n",
        "new_df = pd.DataFrame([])\n",
        "new_df['flow_size'] = test_mini_df['flow_size']\n",
        "new_df['job'] = test_mini_df['job']\n",
        "new_df['predicted_flow_size'] = test_mini_df['flow_size']\n",
        "\n",
        "#now copy predicted\n",
        "for index, row in test_mini_df.iterrows():\n",
        "  i = int(row['pseudo_index'])\n",
        "  new_df.iloc[index,2] = unscaled_test_df.iloc[i]['flow_size']\n",
        "\n",
        "new_df.to_csv('KMeans_test_jb_mini_predicted_no_outliers_GBDT.csv')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKxU3PwEKSD4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "8bd5b5e5-45f1-4ab7-dda3-7d5ea01bddb5"
      },
      "source": [
        "#GradientBoosted Decision tree: SGD:\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "np.random.seed(1)     \n",
        "\n",
        "\n",
        "def add_context(data_df, context = 0):\n",
        "\n",
        "    L = len(data_df)\n",
        "    cols = list(data_df.columns)\n",
        "\n",
        "    context_data = []\n",
        "    \n",
        "    row_1 = np.asarray(data_df.iloc[0,:])\n",
        "    row_1 = np.reshape(row_1,(1,-1))\n",
        "    row_1 = pd.DataFrame(row_1, columns=cols)\n",
        "    #pad data with data[0] context times\n",
        "    data = pd.concat([row_1]*context + [data_df])\n",
        "    \n",
        "    try:\n",
        "        del data[0]\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    data = data.to_numpy()\n",
        "    \n",
        "    \n",
        "    for fi in range(context, len(data)):\n",
        "        frame = data[fi-context:fi+1].flatten()\n",
        "        context_data.append(frame)\n",
        "\n",
        "    return pd.DataFrame(context_data, columns = cols*(context+1))\n",
        "\n",
        "def read_dataset(filename):\n",
        "    dataset = pd.read_csv(filename)\n",
        "    return dataset\n",
        "\n",
        "def get_data_labels(df, context = 0):\n",
        "    label = df['flow_size']\n",
        "    try:\n",
        "      del df['job']\n",
        "    except:\n",
        "      pass\n",
        "    df = add_context(df, context)\n",
        "    df.iloc[:, -1] = 0\n",
        "    #add context to labels\n",
        "    return df, label\n",
        "\n",
        "\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "train_df = read_dataset('SGD_training.csv')\n",
        "test_df = read_dataset('SGD_test.csv')\n",
        "\n",
        "#context = 4\n",
        "context=1\n",
        "\n",
        "cols = train_df.columns\n",
        "\n",
        "#scale and convert to df\n",
        "#fit on train, transform train, test, validation\n",
        "train_df = scaler.fit_transform(train_df)\n",
        "train_df = pd.DataFrame(train_df, columns = cols)\n",
        "\n",
        "test_df = scaler.transform(test_df)\n",
        "test_df = pd.DataFrame(test_df, columns = cols)\n",
        "\n",
        "\n",
        "training_x, training_y = get_data_labels(train_df, context)\n",
        "testing_x, testing_y = get_data_labels(test_df, context)\n",
        "\n",
        "\n",
        "regItem = GradientBoostingRegressor( max_depth=10, n_estimators=50, learning_rate=1.0 )\n",
        "regItem.fit(training_x, training_y)\n",
        "list_of_err=[]\n",
        "\n",
        "for predicted_y in regItem.staged_predict(testing_x):\n",
        "    list_of_err.append(mean_squared_error(testing_y, predicted_y))\n",
        "topEst = np.argmin(list_of_err)\n",
        "\n",
        "bestRegItem = GradientBoostingRegressor( max_depth=7, n_estimators=topEst+1, learning_rate=1.0 )\n",
        "bestRegItem.fit(training_x, training_y)\n",
        "#predicted_score = bestRegItem.score(testing_x,testing_y)\n",
        "#predicted_y = bestRegItem.predict(testing_x)\n",
        "\n",
        "predicted_score = bestRegItem.score(testing_x,testing_y)\n",
        "predicted_y = bestRegItem.predict(testing_x)\n",
        "\n",
        "print(\"Accuracy is \",predicted_score)\n",
        "print(\"R2 is \", r2_score(testing_y, predicted_y))\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is  0.6110783307902424\n",
            "R2 is  0.6110783307902424\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR7FKy_MKbLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_mini_df = read_dataset('SGD_test_jb_mini.csv')\n",
        "#jobs = test_mini_df['job']\n",
        "#del test_mini_df['job']\n",
        "\n",
        "#get original test_df\n",
        "test_df['flow_size'] = predicted_y\n",
        "#print(test_df.tail())\n",
        "unscaled_test_df = scaler.inverse_transform(test_df)\n",
        "unscaled_test_df = pd.DataFrame(unscaled_test_df, columns = cols)\n",
        "#change_cols = {\"agg_net_out\":'int32', \"agg_net_in\":'int32', \"agg_net_out_per_machine\":'int32','agg_net_in_per_machine':'int32','machine':'int32','flow_size':'int32'}\n",
        "#unscaled_test_df = unscaled_test_df.astype(change_cols)\n",
        "unscaled_test_df.tail(20)\n",
        "\n",
        "new_df = pd.DataFrame([])\n",
        "new_df['flow_size'] = test_mini_df['flow_size']\n",
        "new_df['job'] = test_mini_df['job']\n",
        "new_df['predicted_flow_size'] = test_mini_df['flow_size']\n",
        "\n",
        "#now copy predicted\n",
        "for index, row in test_mini_df.iterrows():\n",
        "  i = int(row['pseudo_index'])\n",
        "  new_df.iloc[index,2] = unscaled_test_df.iloc[i]['flow_size']\n",
        "\n",
        "new_df.to_csv('SGD_test_jb_mini_predicted_no_outliers_GBDT.csv')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nucoaP0XKgC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}