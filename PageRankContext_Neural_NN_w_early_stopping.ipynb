{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PageRankContext_Neural_NN_w_early_stopping.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5U7t4YqnxgG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# In[4]:\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "# In[5]:\n",
        "\n",
        "\n",
        "import keras\n",
        "\n",
        "\n",
        "# In[61]:\n",
        "\n",
        "\n",
        "# Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, LSTM, Dropout, Activation\n",
        "from keras import backend as K\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "np.random.seed(1)     \n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6q9OecKoAVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#change based on type of task\n",
        "\n",
        "#TEST_NAME = \"KMeans\"\n",
        "TEST_NAME = \"PageRank\"\n",
        "#TEST_NAME = \"SGD\"\n",
        "\n",
        "#may be needed\n",
        "TEST_PATH = ''\n",
        "#TEST_PATH = '_jb_mini_normal'\n",
        "\n",
        "#plt.style.use('ggplot')\n",
        "\n",
        "def plot_history(history):\n",
        "    '''\n",
        "    used to plot loss curve for model training\n",
        "    '''\n",
        "    #acc = history.history['r2_score_other']\n",
        "    #val_acc = history.history['val_acc']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(loss) + 1)\n",
        "\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "# In[42]:\n",
        "\n",
        "def add_context(data_df, context = 0):\n",
        "    '''\n",
        "    context: how many previous frames to add\n",
        "\n",
        "    Adds (context-1) observations before each i'th observation\n",
        "    '''\n",
        "    L = len(data_df)\n",
        "    cols = list(data_df.columns)\n",
        "\n",
        "    context_data = []\n",
        "    \n",
        "    row_1 = np.asarray(data_df.iloc[0,:])\n",
        "    row_1 = np.reshape(row_1,(1,-1))\n",
        "    row_1 = pd.DataFrame(row_1, columns=cols)\n",
        "    #pad data with data[0] context times\n",
        "    data = pd.concat([row_1]*context + [data_df])\n",
        "    \n",
        "    try:\n",
        "        del data[0]\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    data = data.to_numpy()\n",
        "    \n",
        "    \n",
        "    for fi in range(context, len(data)):\n",
        "        frame = data[fi-context:fi+1].flatten()\n",
        "        context_data.append(frame)\n",
        "    \n",
        "    return pd.DataFrame(context_data, columns = cols*(context+1))\n",
        "\n",
        "def read_dataset(filename):\n",
        "    dataset = pd.read_csv(filename)\n",
        "    return dataset\n",
        "\n",
        "def get_data_labels(df, context = 0):\n",
        "    '''\n",
        "    Separate into features and labels\n",
        "    '''\n",
        "    label = df['flow_size']\n",
        "    try:\n",
        "      del df['job']\n",
        "    except:\n",
        "      pass\n",
        "    df = add_context(df, context)\n",
        "    df.iloc[:, -1] = 0\n",
        "    #add context to labels\n",
        "    return df, label\n",
        "\n",
        "#r2 score used in paper. 1 is best. -1 is bad.\n",
        "\n",
        "def r2_score_other(y_true, y_pred):\n",
        "    from keras import backend as K\n",
        "    SS_res =  K.sum(K.square( y_true-y_pred ))\n",
        "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
        "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
        "\n",
        "\n",
        "#from https://stackoverflow.com/questions/45250100/kerasregressor-coefficient-of-determination-r2-score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5ePTgxooHuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#need to scale values to the range(0,1)\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "\n",
        "train_df = read_dataset(TEST_NAME+'_training' + TEST_PATH + '.csv')\n",
        "test_df = read_dataset(TEST_NAME+'_test' + TEST_PATH + '.csv')\n",
        "validation_df = read_dataset(TEST_NAME+'_validation' + TEST_PATH + '.csv')\n",
        "\n",
        "#all will have same columns\n",
        "cols = train_df.columns\n",
        "\n",
        "#scale and convert to df\n",
        "#fit on train, transform train, test, validation\n",
        "train_df = scaler.fit_transform(train_df)\n",
        "train_df = pd.DataFrame(train_df, columns = cols)\n",
        "\n",
        "test_df = scaler.transform(test_df)\n",
        "test_df = pd.DataFrame(test_df, columns = cols)\n",
        "\n",
        "validation_df = scaler.transform(validation_df)\n",
        "validation_df = pd.DataFrame(validation_df, columns = cols)\n",
        "\n",
        "context = 1\n",
        "\n",
        "train_x, train_y = get_data_labels(train_df, context)\n",
        "test_x, test_y = get_data_labels(test_df, context)\n",
        "validation_x, validation_y = get_data_labels(validation_df, context)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcvJZ6NO097T",
        "colab_type": "code",
        "outputId": "16d768cc-e80c-42f1-d3a5-ba30a0c196e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_x.shape, train_y.shape"
      ],
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((42629, 16), (42629,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 274
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfuz--xvOGwM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define callback for early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', mode='min', patience=50)\n",
        "\n",
        "#define callback for saving best model\n",
        "model_name = 'nn_model_' + TEST_NAME + str(context) + 'context' + '.h5'\n",
        "best_model = ModelCheckpoint(model_name, monitor='val_loss',mode='min')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-j7xHv3oMV-",
        "colab_type": "code",
        "outputId": "ddcb5f2f-aa6d-43c9-8fc2-13a846f0babf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# In[102]:\n",
        "model = Sequential()\n",
        "model.add(Dense(5, input_dim=train_x.shape[1], activation='relu'))\n",
        "model.add(Dense(5, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='mean_absolute_error', optimizer='adam')\n",
        "history = model.fit(train_x, train_y, epochs=250, \\\n",
        "                      validation_data = (validation_x, validation_y), \\\n",
        "                      batch_size=10, verbose=2, callbacks = [early_stop, best_model])\n",
        "\n",
        "#plot_history(history)\n",
        "\n",
        "#error metric\n",
        "model.evaluate(test_x, test_y)"
      ],
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42629 samples, validate on 12552 samples\n",
            "Epoch 1/250\n",
            " - 4s - loss: 0.0726 - val_loss: 0.0513\n",
            "Epoch 2/250\n",
            " - 4s - loss: 0.0603 - val_loss: 0.0510\n",
            "Epoch 3/250\n",
            " - 4s - loss: 0.0602 - val_loss: 0.0510\n",
            "Epoch 4/250\n",
            " - 4s - loss: 0.0601 - val_loss: 0.0508\n",
            "Epoch 5/250\n",
            " - 4s - loss: 0.0600 - val_loss: 0.0508\n",
            "Epoch 6/250\n",
            " - 4s - loss: 0.0599 - val_loss: 0.0507\n",
            "Epoch 7/250\n",
            " - 4s - loss: 0.0598 - val_loss: 0.0509\n",
            "Epoch 8/250\n",
            " - 4s - loss: 0.0598 - val_loss: 0.0506\n",
            "Epoch 9/250\n",
            " - 4s - loss: 0.0597 - val_loss: 0.0507\n",
            "Epoch 10/250\n",
            " - 4s - loss: 0.0596 - val_loss: 0.0505\n",
            "Epoch 11/250\n",
            " - 4s - loss: 0.0595 - val_loss: 0.0507\n",
            "Epoch 12/250\n",
            " - 4s - loss: 0.0594 - val_loss: 0.0503\n",
            "Epoch 13/250\n",
            " - 4s - loss: 0.0592 - val_loss: 0.0502\n",
            "Epoch 14/250\n",
            " - 4s - loss: 0.0591 - val_loss: 0.0503\n",
            "Epoch 15/250\n",
            " - 4s - loss: 0.0591 - val_loss: 0.0501\n",
            "Epoch 16/250\n",
            " - 4s - loss: 0.0590 - val_loss: 0.0502\n",
            "Epoch 17/250\n",
            " - 4s - loss: 0.0589 - val_loss: 0.0500\n",
            "Epoch 18/250\n",
            " - 4s - loss: 0.0588 - val_loss: 0.0499\n",
            "Epoch 19/250\n",
            " - 4s - loss: 0.0587 - val_loss: 0.0499\n",
            "Epoch 20/250\n",
            " - 4s - loss: 0.0587 - val_loss: 0.0500\n",
            "Epoch 21/250\n",
            " - 5s - loss: 0.0586 - val_loss: 0.0499\n",
            "Epoch 22/250\n",
            " - 4s - loss: 0.0584 - val_loss: 0.0498\n",
            "Epoch 23/250\n",
            " - 4s - loss: 0.0583 - val_loss: 0.0499\n",
            "Epoch 24/250\n",
            " - 4s - loss: 0.0582 - val_loss: 0.0499\n",
            "Epoch 25/250\n",
            " - 4s - loss: 0.0580 - val_loss: 0.0498\n",
            "Epoch 26/250\n",
            " - 4s - loss: 0.0580 - val_loss: 0.0495\n",
            "Epoch 27/250\n",
            " - 4s - loss: 0.0578 - val_loss: 0.0494\n",
            "Epoch 28/250\n",
            " - 4s - loss: 0.0577 - val_loss: 0.0497\n",
            "Epoch 29/250\n",
            " - 4s - loss: 0.0576 - val_loss: 0.0494\n",
            "Epoch 30/250\n",
            " - 4s - loss: 0.0575 - val_loss: 0.0495\n",
            "Epoch 31/250\n",
            " - 4s - loss: 0.0575 - val_loss: 0.0495\n",
            "Epoch 32/250\n",
            " - 4s - loss: 0.0574 - val_loss: 0.0492\n",
            "Epoch 33/250\n",
            " - 4s - loss: 0.0573 - val_loss: 0.0493\n",
            "Epoch 34/250\n",
            " - 4s - loss: 0.0573 - val_loss: 0.0494\n",
            "Epoch 35/250\n",
            " - 4s - loss: 0.0572 - val_loss: 0.0491\n",
            "Epoch 36/250\n",
            " - 4s - loss: 0.0572 - val_loss: 0.0494\n",
            "Epoch 37/250\n",
            " - 4s - loss: 0.0572 - val_loss: 0.0491\n",
            "Epoch 38/250\n",
            " - 4s - loss: 0.0570 - val_loss: 0.0492\n",
            "Epoch 39/250\n",
            " - 4s - loss: 0.0569 - val_loss: 0.0491\n",
            "Epoch 40/250\n",
            " - 4s - loss: 0.0569 - val_loss: 0.0489\n",
            "Epoch 41/250\n",
            " - 4s - loss: 0.0568 - val_loss: 0.0489\n",
            "Epoch 42/250\n",
            " - 4s - loss: 0.0568 - val_loss: 0.0490\n",
            "Epoch 43/250\n",
            " - 4s - loss: 0.0567 - val_loss: 0.0488\n",
            "Epoch 44/250\n",
            " - 4s - loss: 0.0567 - val_loss: 0.0492\n",
            "Epoch 45/250\n",
            " - 4s - loss: 0.0567 - val_loss: 0.0487\n",
            "Epoch 46/250\n",
            " - 4s - loss: 0.0565 - val_loss: 0.0491\n",
            "Epoch 47/250\n",
            " - 4s - loss: 0.0565 - val_loss: 0.0488\n",
            "Epoch 48/250\n",
            " - 5s - loss: 0.0565 - val_loss: 0.0487\n",
            "Epoch 49/250\n",
            " - 4s - loss: 0.0564 - val_loss: 0.0487\n",
            "Epoch 50/250\n",
            " - 4s - loss: 0.0563 - val_loss: 0.0487\n",
            "Epoch 51/250\n",
            " - 4s - loss: 0.0562 - val_loss: 0.0486\n",
            "Epoch 52/250\n",
            " - 4s - loss: 0.0561 - val_loss: 0.0487\n",
            "Epoch 53/250\n",
            " - 4s - loss: 0.0561 - val_loss: 0.0484\n",
            "Epoch 54/250\n",
            " - 4s - loss: 0.0561 - val_loss: 0.0480\n",
            "Epoch 55/250\n",
            " - 4s - loss: 0.0559 - val_loss: 0.0490\n",
            "Epoch 56/250\n",
            " - 4s - loss: 0.0560 - val_loss: 0.0486\n",
            "Epoch 57/250\n",
            " - 4s - loss: 0.0560 - val_loss: 0.0489\n",
            "Epoch 58/250\n",
            " - 4s - loss: 0.0559 - val_loss: 0.0481\n",
            "Epoch 59/250\n",
            " - 4s - loss: 0.0558 - val_loss: 0.0485\n",
            "Epoch 60/250\n",
            " - 4s - loss: 0.0559 - val_loss: 0.0483\n",
            "Epoch 61/250\n",
            " - 4s - loss: 0.0559 - val_loss: 0.0482\n",
            "Epoch 62/250\n",
            " - 4s - loss: 0.0558 - val_loss: 0.0484\n",
            "Epoch 63/250\n",
            " - 4s - loss: 0.0558 - val_loss: 0.0478\n",
            "Epoch 64/250\n",
            " - 5s - loss: 0.0558 - val_loss: 0.0482\n",
            "Epoch 65/250\n",
            " - 5s - loss: 0.0558 - val_loss: 0.0490\n",
            "Epoch 66/250\n",
            " - 4s - loss: 0.0558 - val_loss: 0.0478\n",
            "Epoch 67/250\n",
            " - 4s - loss: 0.0558 - val_loss: 0.0477\n",
            "Epoch 68/250\n",
            " - 4s - loss: 0.0556 - val_loss: 0.0476\n",
            "Epoch 69/250\n",
            " - 4s - loss: 0.0557 - val_loss: 0.0483\n",
            "Epoch 70/250\n",
            " - 4s - loss: 0.0557 - val_loss: 0.0473\n",
            "Epoch 71/250\n",
            " - 4s - loss: 0.0557 - val_loss: 0.0478\n",
            "Epoch 72/250\n",
            " - 4s - loss: 0.0555 - val_loss: 0.0477\n",
            "Epoch 73/250\n",
            " - 4s - loss: 0.0557 - val_loss: 0.0474\n",
            "Epoch 74/250\n",
            " - 4s - loss: 0.0555 - val_loss: 0.0473\n",
            "Epoch 75/250\n",
            " - 4s - loss: 0.0555 - val_loss: 0.0472\n",
            "Epoch 76/250\n",
            " - 4s - loss: 0.0555 - val_loss: 0.0474\n",
            "Epoch 77/250\n",
            " - 4s - loss: 0.0555 - val_loss: 0.0472\n",
            "Epoch 78/250\n",
            " - 4s - loss: 0.0555 - val_loss: 0.0472\n",
            "Epoch 79/250\n",
            " - 4s - loss: 0.0554 - val_loss: 0.0477\n",
            "Epoch 80/250\n",
            " - 4s - loss: 0.0554 - val_loss: 0.0479\n",
            "Epoch 81/250\n",
            " - 4s - loss: 0.0554 - val_loss: 0.0472\n",
            "Epoch 82/250\n",
            " - 4s - loss: 0.0554 - val_loss: 0.0474\n",
            "Epoch 83/250\n",
            " - 4s - loss: 0.0554 - val_loss: 0.0472\n",
            "Epoch 84/250\n",
            " - 4s - loss: 0.0554 - val_loss: 0.0472\n",
            "Epoch 85/250\n",
            " - 4s - loss: 0.0554 - val_loss: 0.0474\n",
            "Epoch 86/250\n",
            " - 4s - loss: 0.0553 - val_loss: 0.0483\n",
            "Epoch 87/250\n",
            " - 4s - loss: 0.0554 - val_loss: 0.0469\n",
            "Epoch 88/250\n",
            " - 4s - loss: 0.0554 - val_loss: 0.0471\n",
            "Epoch 89/250\n",
            " - 5s - loss: 0.0553 - val_loss: 0.0474\n",
            "Epoch 90/250\n",
            " - 5s - loss: 0.0554 - val_loss: 0.0473\n",
            "Epoch 91/250\n",
            " - 4s - loss: 0.0553 - val_loss: 0.0471\n",
            "Epoch 92/250\n",
            " - 4s - loss: 0.0553 - val_loss: 0.0475\n",
            "Epoch 93/250\n",
            " - 4s - loss: 0.0553 - val_loss: 0.0471\n",
            "Epoch 94/250\n",
            " - 4s - loss: 0.0553 - val_loss: 0.0470\n",
            "Epoch 95/250\n",
            " - 4s - loss: 0.0552 - val_loss: 0.0473\n",
            "Epoch 96/250\n",
            " - 4s - loss: 0.0553 - val_loss: 0.0473\n",
            "Epoch 97/250\n",
            " - 4s - loss: 0.0553 - val_loss: 0.0472\n",
            "Epoch 98/250\n",
            " - 4s - loss: 0.0552 - val_loss: 0.0473\n",
            "Epoch 99/250\n",
            " - 4s - loss: 0.0551 - val_loss: 0.0474\n",
            "Epoch 100/250\n",
            " - 4s - loss: 0.0551 - val_loss: 0.0479\n",
            "Epoch 101/250\n",
            " - 4s - loss: 0.0552 - val_loss: 0.0470\n",
            "Epoch 102/250\n",
            " - 4s - loss: 0.0552 - val_loss: 0.0471\n",
            "Epoch 103/250\n",
            " - 5s - loss: 0.0551 - val_loss: 0.0469\n",
            "Epoch 104/250\n",
            " - 4s - loss: 0.0553 - val_loss: 0.0471\n",
            "Epoch 105/250\n",
            " - 4s - loss: 0.0552 - val_loss: 0.0470\n",
            "Epoch 106/250\n",
            " - 4s - loss: 0.0551 - val_loss: 0.0471\n",
            "Epoch 107/250\n",
            " - 4s - loss: 0.0550 - val_loss: 0.0469\n",
            "Epoch 108/250\n",
            " - 4s - loss: 0.0551 - val_loss: 0.0475\n",
            "Epoch 109/250\n",
            " - 4s - loss: 0.0552 - val_loss: 0.0469\n",
            "Epoch 110/250\n",
            " - 4s - loss: 0.0550 - val_loss: 0.0471\n",
            "Epoch 111/250\n",
            " - 4s - loss: 0.0551 - val_loss: 0.0469\n",
            "Epoch 112/250\n",
            " - 4s - loss: 0.0551 - val_loss: 0.0470\n",
            "Epoch 113/250\n",
            " - 4s - loss: 0.0550 - val_loss: 0.0470\n",
            "Epoch 114/250\n",
            " - 4s - loss: 0.0550 - val_loss: 0.0472\n",
            "Epoch 115/250\n",
            " - 4s - loss: 0.0550 - val_loss: 0.0468\n",
            "Epoch 116/250\n",
            " - 4s - loss: 0.0550 - val_loss: 0.0473\n",
            "Epoch 117/250\n",
            " - 4s - loss: 0.0550 - val_loss: 0.0470\n",
            "Epoch 118/250\n",
            " - 4s - loss: 0.0550 - val_loss: 0.0469\n",
            "Epoch 119/250\n",
            " - 4s - loss: 0.0550 - val_loss: 0.0468\n",
            "Epoch 120/250\n",
            " - 4s - loss: 0.0549 - val_loss: 0.0469\n",
            "Epoch 121/250\n",
            " - 4s - loss: 0.0549 - val_loss: 0.0471\n",
            "Epoch 122/250\n",
            " - 4s - loss: 0.0550 - val_loss: 0.0468\n",
            "Epoch 123/250\n",
            " - 4s - loss: 0.0550 - val_loss: 0.0469\n",
            "Epoch 124/250\n",
            " - 4s - loss: 0.0549 - val_loss: 0.0470\n",
            "Epoch 125/250\n",
            " - 4s - loss: 0.0549 - val_loss: 0.0467\n",
            "Epoch 126/250\n",
            " - 4s - loss: 0.0549 - val_loss: 0.0471\n",
            "Epoch 127/250\n",
            " - 4s - loss: 0.0549 - val_loss: 0.0469\n",
            "Epoch 128/250\n",
            " - 4s - loss: 0.0548 - val_loss: 0.0471\n",
            "Epoch 129/250\n",
            " - 4s - loss: 0.0549 - val_loss: 0.0468\n",
            "Epoch 130/250\n",
            " - 4s - loss: 0.0548 - val_loss: 0.0472\n",
            "Epoch 131/250\n",
            " - 4s - loss: 0.0548 - val_loss: 0.0467\n",
            "Epoch 132/250\n",
            " - 4s - loss: 0.0548 - val_loss: 0.0467\n",
            "Epoch 133/250\n",
            " - 4s - loss: 0.0548 - val_loss: 0.0469\n",
            "Epoch 134/250\n",
            " - 5s - loss: 0.0548 - val_loss: 0.0467\n",
            "Epoch 135/250\n",
            " - 5s - loss: 0.0547 - val_loss: 0.0468\n",
            "Epoch 136/250\n",
            " - 5s - loss: 0.0548 - val_loss: 0.0466\n",
            "Epoch 137/250\n",
            " - 6s - loss: 0.0547 - val_loss: 0.0469\n",
            "Epoch 138/250\n",
            " - 5s - loss: 0.0548 - val_loss: 0.0467\n",
            "Epoch 139/250\n",
            " - 4s - loss: 0.0548 - val_loss: 0.0468\n",
            "Epoch 140/250\n",
            " - 4s - loss: 0.0547 - val_loss: 0.0469\n",
            "Epoch 141/250\n",
            " - 4s - loss: 0.0547 - val_loss: 0.0466\n",
            "Epoch 142/250\n",
            " - 4s - loss: 0.0548 - val_loss: 0.0466\n",
            "Epoch 143/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0468\n",
            "Epoch 144/250\n",
            " - 4s - loss: 0.0547 - val_loss: 0.0472\n",
            "Epoch 145/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0467\n",
            "Epoch 146/250\n",
            " - 4s - loss: 0.0548 - val_loss: 0.0467\n",
            "Epoch 147/250\n",
            " - 4s - loss: 0.0547 - val_loss: 0.0466\n",
            "Epoch 148/250\n",
            " - 4s - loss: 0.0548 - val_loss: 0.0466\n",
            "Epoch 149/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0467\n",
            "Epoch 150/250\n",
            " - 4s - loss: 0.0547 - val_loss: 0.0469\n",
            "Epoch 151/250\n",
            " - 4s - loss: 0.0547 - val_loss: 0.0469\n",
            "Epoch 152/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0476\n",
            "Epoch 153/250\n",
            " - 4s - loss: 0.0547 - val_loss: 0.0469\n",
            "Epoch 154/250\n",
            " - 4s - loss: 0.0547 - val_loss: 0.0468\n",
            "Epoch 155/250\n",
            " - 4s - loss: 0.0547 - val_loss: 0.0468\n",
            "Epoch 156/250\n",
            " - 4s - loss: 0.0547 - val_loss: 0.0466\n",
            "Epoch 157/250\n",
            " - 4s - loss: 0.0547 - val_loss: 0.0467\n",
            "Epoch 158/250\n",
            " - 5s - loss: 0.0547 - val_loss: 0.0468\n",
            "Epoch 159/250\n",
            " - 5s - loss: 0.0547 - val_loss: 0.0467\n",
            "Epoch 160/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0469\n",
            "Epoch 161/250\n",
            " - 4s - loss: 0.0547 - val_loss: 0.0468\n",
            "Epoch 162/250\n",
            " - 4s - loss: 0.0547 - val_loss: 0.0467\n",
            "Epoch 163/250\n",
            " - 4s - loss: 0.0547 - val_loss: 0.0467\n",
            "Epoch 164/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0465\n",
            "Epoch 165/250\n",
            " - 4s - loss: 0.0547 - val_loss: 0.0464\n",
            "Epoch 166/250\n",
            " - 4s - loss: 0.0547 - val_loss: 0.0465\n",
            "Epoch 167/250\n",
            " - 4s - loss: 0.0547 - val_loss: 0.0465\n",
            "Epoch 168/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0466\n",
            "Epoch 169/250\n",
            " - 4s - loss: 0.0547 - val_loss: 0.0467\n",
            "Epoch 170/250\n",
            " - 4s - loss: 0.0547 - val_loss: 0.0466\n",
            "Epoch 171/250\n",
            " - 4s - loss: 0.0547 - val_loss: 0.0465\n",
            "Epoch 172/250\n",
            " - 4s - loss: 0.0547 - val_loss: 0.0466\n",
            "Epoch 173/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0468\n",
            "Epoch 174/250\n",
            " - 4s - loss: 0.0548 - val_loss: 0.0466\n",
            "Epoch 175/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0468\n",
            "Epoch 176/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0467\n",
            "Epoch 177/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0476\n",
            "Epoch 178/250\n",
            " - 4s - loss: 0.0547 - val_loss: 0.0466\n",
            "Epoch 179/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0465\n",
            "Epoch 180/250\n",
            " - 4s - loss: 0.0547 - val_loss: 0.0467\n",
            "Epoch 181/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0468\n",
            "Epoch 182/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0466\n",
            "Epoch 183/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0466\n",
            "Epoch 184/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0467\n",
            "Epoch 185/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0466\n",
            "Epoch 186/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0471\n",
            "Epoch 187/250\n",
            " - 4s - loss: 0.0547 - val_loss: 0.0468\n",
            "Epoch 188/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0466\n",
            "Epoch 189/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0465\n",
            "Epoch 190/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0468\n",
            "Epoch 191/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0466\n",
            "Epoch 192/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0470\n",
            "Epoch 193/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0468\n",
            "Epoch 194/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0468\n",
            "Epoch 195/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0465\n",
            "Epoch 196/250\n",
            " - 4s - loss: 0.0547 - val_loss: 0.0466\n",
            "Epoch 197/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0465\n",
            "Epoch 198/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0465\n",
            "Epoch 199/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0465\n",
            "Epoch 200/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0465\n",
            "Epoch 201/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0465\n",
            "Epoch 202/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0465\n",
            "Epoch 203/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0467\n",
            "Epoch 204/250\n",
            " - 5s - loss: 0.0545 - val_loss: 0.0465\n",
            "Epoch 205/250\n",
            " - 5s - loss: 0.0545 - val_loss: 0.0466\n",
            "Epoch 206/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0466\n",
            "Epoch 207/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0465\n",
            "Epoch 208/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0465\n",
            "Epoch 209/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0468\n",
            "Epoch 210/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0465\n",
            "Epoch 211/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0465\n",
            "Epoch 212/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0466\n",
            "Epoch 213/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0466\n",
            "Epoch 214/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0464\n",
            "Epoch 215/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0465\n",
            "Epoch 216/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0465\n",
            "Epoch 217/250\n",
            " - 5s - loss: 0.0545 - val_loss: 0.0466\n",
            "Epoch 218/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0465\n",
            "Epoch 219/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0463\n",
            "Epoch 220/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0463\n",
            "Epoch 221/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0466\n",
            "Epoch 222/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0466\n",
            "Epoch 223/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0466\n",
            "Epoch 224/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0466\n",
            "Epoch 225/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0465\n",
            "Epoch 226/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0466\n",
            "Epoch 227/250\n",
            " - 5s - loss: 0.0546 - val_loss: 0.0464\n",
            "Epoch 228/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0465\n",
            "Epoch 229/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0465\n",
            "Epoch 230/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0468\n",
            "Epoch 231/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0465\n",
            "Epoch 232/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0466\n",
            "Epoch 233/250\n",
            " - 4s - loss: 0.0544 - val_loss: 0.0465\n",
            "Epoch 234/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0467\n",
            "Epoch 235/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0469\n",
            "Epoch 236/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0464\n",
            "Epoch 237/250\n",
            " - 4s - loss: 0.0544 - val_loss: 0.0465\n",
            "Epoch 238/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0465\n",
            "Epoch 239/250\n",
            " - 4s - loss: 0.0544 - val_loss: 0.0468\n",
            "Epoch 240/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0465\n",
            "Epoch 241/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0466\n",
            "Epoch 242/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0463\n",
            "Epoch 243/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0465\n",
            "Epoch 244/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0466\n",
            "Epoch 245/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0467\n",
            "Epoch 246/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0466\n",
            "Epoch 247/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0463\n",
            "Epoch 248/250\n",
            " - 4s - loss: 0.0545 - val_loss: 0.0463\n",
            "Epoch 249/250\n",
            " - 4s - loss: 0.0546 - val_loss: 0.0464\n",
            "Epoch 250/250\n",
            " - 4s - loss: 0.0544 - val_loss: 0.0464\n",
            "12091/12091 [==============================] - 0s 16us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.046240692532281406"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 277
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJc82JNX_SKs",
        "colab_type": "code",
        "outputId": "2a088793-070f-4452-f9e5-a9d0018e6222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3GHvnnVoOVV",
        "colab_type": "code",
        "outputId": "0b8244c2-c064-48d9-9a70-58e99c57e992",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#save final model in addition with the best model (may be same)\n",
        "model.save('final'+model_name)\n",
        "\n",
        "pred_y = model.predict(test_x)\n",
        "score = r2_score(test_y, pred_y)\n",
        "print(f'Test r2 score: {score}')\n"
      ],
      "execution_count": 280,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test r2 score: 0.1968850228023884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWZNK1BAvdH-",
        "colab_type": "code",
        "outputId": "da38418c-f900-47a6-f1e8-c6f427da646d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#for extra training, if required\n",
        "'''\n",
        "history = model.fit(train_x, train_y, epochs=100, \\\n",
        "                      validation_data = (validation_x, validation_y), \\\n",
        "                      batch_size=10, verbose=2, callbacks = [early_stop, best_model])\n",
        "\n",
        "pred_y = model.predict(test_x)\n",
        "score = r2_score(test_y, pred_y)\n",
        "print(f'Test r2 score: {score}')\n",
        "'''"
      ],
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 54646 samples, validate on 15695 samples\n",
            "Epoch 1/100\n",
            " - 6s - loss: 8.8314e-04 - val_loss: 8.4001e-04\n",
            "Epoch 2/100\n",
            " - 6s - loss: 8.8094e-04 - val_loss: 8.5468e-04\n",
            "Epoch 3/100\n",
            " - 6s - loss: 8.7457e-04 - val_loss: 8.8203e-04\n",
            "Epoch 4/100\n",
            " - 6s - loss: 8.9701e-04 - val_loss: 8.7858e-04\n",
            "Epoch 5/100\n",
            " - 6s - loss: 9.0467e-04 - val_loss: 9.0176e-04\n",
            "Epoch 6/100\n",
            " - 6s - loss: 8.8233e-04 - val_loss: 8.4948e-04\n",
            "Epoch 7/100\n",
            " - 6s - loss: 8.8587e-04 - val_loss: 8.6156e-04\n",
            "Epoch 8/100\n",
            " - 6s - loss: 8.7894e-04 - val_loss: 9.1089e-04\n",
            "Epoch 9/100\n",
            " - 6s - loss: 8.9530e-04 - val_loss: 9.3459e-04\n",
            "Epoch 10/100\n",
            " - 6s - loss: 8.7954e-04 - val_loss: 8.5337e-04\n",
            "Epoch 11/100\n",
            " - 6s - loss: 8.9358e-04 - val_loss: 8.7116e-04\n",
            "Epoch 12/100\n",
            " - 6s - loss: 8.6110e-04 - val_loss: 8.5881e-04\n",
            "Epoch 13/100\n",
            " - 6s - loss: 8.7985e-04 - val_loss: 9.7765e-04\n",
            "Epoch 14/100\n",
            " - 6s - loss: 8.8875e-04 - val_loss: 8.6442e-04\n",
            "Epoch 15/100\n",
            " - 6s - loss: 8.7354e-04 - val_loss: 8.6909e-04\n",
            "Epoch 16/100\n",
            " - 6s - loss: 9.0084e-04 - val_loss: 8.5814e-04\n",
            "Epoch 17/100\n",
            " - 6s - loss: 8.7455e-04 - val_loss: 8.7863e-04\n",
            "Epoch 18/100\n",
            " - 6s - loss: 8.7366e-04 - val_loss: 8.5916e-04\n",
            "Epoch 19/100\n",
            " - 6s - loss: 8.8060e-04 - val_loss: 8.5890e-04\n",
            "Epoch 20/100\n",
            " - 6s - loss: 8.8149e-04 - val_loss: 8.6696e-04\n",
            "Epoch 21/100\n",
            " - 6s - loss: 8.7564e-04 - val_loss: 8.7364e-04\n",
            "Epoch 22/100\n",
            " - 6s - loss: 8.9369e-04 - val_loss: 8.5620e-04\n",
            "Epoch 23/100\n",
            " - 6s - loss: 8.8995e-04 - val_loss: 8.7726e-04\n",
            "Epoch 24/100\n",
            " - 6s - loss: 8.7868e-04 - val_loss: 8.6884e-04\n",
            "Epoch 25/100\n",
            " - 6s - loss: 8.8331e-04 - val_loss: 8.6362e-04\n",
            "Epoch 26/100\n",
            " - 6s - loss: 8.8359e-04 - val_loss: 9.1387e-04\n",
            "Epoch 27/100\n",
            " - 6s - loss: 8.8492e-04 - val_loss: 8.4868e-04\n",
            "Epoch 28/100\n",
            " - 6s - loss: 8.9569e-04 - val_loss: 8.7688e-04\n",
            "Epoch 29/100\n",
            " - 6s - loss: 8.7494e-04 - val_loss: 8.7744e-04\n",
            "Epoch 30/100\n",
            " - 6s - loss: 8.7208e-04 - val_loss: 8.7938e-04\n",
            "Epoch 31/100\n",
            " - 6s - loss: 8.8319e-04 - val_loss: 8.3992e-04\n",
            "Epoch 32/100\n",
            " - 6s - loss: 8.8991e-04 - val_loss: 8.5990e-04\n",
            "Epoch 33/100\n",
            " - 6s - loss: 8.6719e-04 - val_loss: 8.6693e-04\n",
            "Epoch 34/100\n",
            " - 6s - loss: 8.6772e-04 - val_loss: 8.4756e-04\n",
            "Epoch 35/100\n",
            " - 6s - loss: 8.6987e-04 - val_loss: 8.8050e-04\n",
            "Epoch 36/100\n",
            " - 6s - loss: 8.6944e-04 - val_loss: 8.6568e-04\n",
            "Epoch 37/100\n",
            " - 6s - loss: 8.6776e-04 - val_loss: 8.5390e-04\n",
            "Epoch 38/100\n",
            " - 6s - loss: 8.8326e-04 - val_loss: 8.6751e-04\n",
            "Epoch 39/100\n",
            " - 6s - loss: 8.6803e-04 - val_loss: 8.5254e-04\n",
            "Epoch 40/100\n",
            " - 6s - loss: 8.8438e-04 - val_loss: 8.5246e-04\n",
            "Epoch 41/100\n",
            " - 6s - loss: 8.8615e-04 - val_loss: 8.5501e-04\n",
            "Epoch 42/100\n",
            " - 6s - loss: 8.8067e-04 - val_loss: 8.5552e-04\n",
            "Epoch 43/100\n",
            " - 6s - loss: 8.5587e-04 - val_loss: 8.7147e-04\n",
            "Epoch 44/100\n",
            " - 6s - loss: 8.7524e-04 - val_loss: 8.4903e-04\n",
            "Epoch 45/100\n",
            " - 6s - loss: 8.7634e-04 - val_loss: 8.5006e-04\n",
            "Epoch 46/100\n",
            " - 6s - loss: 8.8176e-04 - val_loss: 8.3574e-04\n",
            "Epoch 47/100\n",
            " - 6s - loss: 8.8295e-04 - val_loss: 8.9335e-04\n",
            "Epoch 48/100\n",
            " - 6s - loss: 8.7359e-04 - val_loss: 8.6648e-04\n",
            "Epoch 49/100\n",
            " - 6s - loss: 8.6652e-04 - val_loss: 8.5558e-04\n",
            "Epoch 50/100\n",
            " - 6s - loss: 8.9292e-04 - val_loss: 8.5364e-04\n",
            "Epoch 51/100\n",
            " - 6s - loss: 8.7746e-04 - val_loss: 8.9250e-04\n",
            "Epoch 52/100\n",
            " - 6s - loss: 9.1680e-04 - val_loss: 8.6264e-04\n",
            "Epoch 53/100\n",
            " - 6s - loss: 8.7469e-04 - val_loss: 9.1109e-04\n",
            "Epoch 54/100\n",
            " - 6s - loss: 8.6658e-04 - val_loss: 8.5485e-04\n",
            "Epoch 55/100\n",
            " - 6s - loss: 8.6643e-04 - val_loss: 8.8252e-04\n",
            "Epoch 56/100\n",
            " - 6s - loss: 8.7102e-04 - val_loss: 8.5777e-04\n",
            "Epoch 57/100\n",
            " - 6s - loss: 8.7663e-04 - val_loss: 8.5275e-04\n",
            "Epoch 58/100\n",
            " - 6s - loss: 8.5432e-04 - val_loss: 8.3618e-04\n",
            "Epoch 59/100\n",
            " - 6s - loss: 9.0359e-04 - val_loss: 9.0998e-04\n",
            "Epoch 60/100\n",
            " - 6s - loss: 8.9024e-04 - val_loss: 8.6715e-04\n",
            "Epoch 61/100\n",
            " - 6s - loss: 8.8542e-04 - val_loss: 8.5749e-04\n",
            "Epoch 62/100\n",
            " - 6s - loss: 8.6846e-04 - val_loss: 8.8501e-04\n",
            "Epoch 63/100\n",
            " - 6s - loss: 8.6698e-04 - val_loss: 8.5111e-04\n",
            "Epoch 64/100\n",
            " - 6s - loss: 8.5914e-04 - val_loss: 8.4893e-04\n",
            "Epoch 65/100\n",
            " - 6s - loss: 8.6257e-04 - val_loss: 8.6908e-04\n",
            "Epoch 66/100\n",
            " - 6s - loss: 8.7843e-04 - val_loss: 8.8832e-04\n",
            "Epoch 67/100\n",
            " - 6s - loss: 8.6805e-04 - val_loss: 8.4277e-04\n",
            "Epoch 68/100\n",
            " - 6s - loss: 8.8416e-04 - val_loss: 8.6447e-04\n",
            "Epoch 69/100\n",
            " - 6s - loss: 8.8263e-04 - val_loss: 8.7402e-04\n",
            "Epoch 70/100\n",
            " - 6s - loss: 8.7971e-04 - val_loss: 8.6887e-04\n",
            "Epoch 71/100\n",
            " - 6s - loss: 8.6105e-04 - val_loss: 9.1220e-04\n",
            "Epoch 72/100\n",
            " - 6s - loss: 8.8013e-04 - val_loss: 8.7640e-04\n",
            "Epoch 73/100\n",
            " - 6s - loss: 8.8315e-04 - val_loss: 9.0605e-04\n",
            "Epoch 74/100\n",
            " - 6s - loss: 9.4508e-04 - val_loss: 8.4970e-04\n",
            "Epoch 75/100\n",
            " - 6s - loss: 8.6527e-04 - val_loss: 8.7509e-04\n",
            "Epoch 76/100\n",
            " - 6s - loss: 8.9925e-04 - val_loss: 8.4031e-04\n",
            "Epoch 77/100\n",
            " - 6s - loss: 8.5889e-04 - val_loss: 8.7227e-04\n",
            "Epoch 78/100\n",
            " - 6s - loss: 8.7442e-04 - val_loss: 8.9250e-04\n",
            "Epoch 79/100\n",
            " - 6s - loss: 8.6467e-04 - val_loss: 8.6099e-04\n",
            "Epoch 80/100\n",
            " - 6s - loss: 8.7841e-04 - val_loss: 8.5309e-04\n",
            "Epoch 81/100\n",
            " - 6s - loss: 8.5197e-04 - val_loss: 8.8345e-04\n",
            "Epoch 82/100\n",
            " - 6s - loss: 8.7371e-04 - val_loss: 8.4046e-04\n",
            "Epoch 83/100\n",
            " - 6s - loss: 8.7504e-04 - val_loss: 8.3515e-04\n",
            "Epoch 84/100\n",
            " - 6s - loss: 8.6356e-04 - val_loss: 8.4065e-04\n",
            "Epoch 85/100\n",
            " - 6s - loss: 8.5282e-04 - val_loss: 8.5434e-04\n",
            "Epoch 86/100\n",
            " - 6s - loss: 8.6524e-04 - val_loss: 8.6822e-04\n",
            "Epoch 87/100\n",
            " - 6s - loss: 8.6857e-04 - val_loss: 8.5695e-04\n",
            "Epoch 88/100\n",
            " - 6s - loss: 8.7117e-04 - val_loss: 8.5605e-04\n",
            "Epoch 89/100\n",
            " - 6s - loss: 8.6369e-04 - val_loss: 8.6321e-04\n",
            "Epoch 90/100\n",
            " - 6s - loss: 8.5776e-04 - val_loss: 8.6260e-04\n",
            "Epoch 91/100\n",
            " - 6s - loss: 8.6705e-04 - val_loss: 8.5786e-04\n",
            "Epoch 92/100\n",
            " - 6s - loss: 8.5859e-04 - val_loss: 8.9110e-04\n",
            "Epoch 93/100\n",
            " - 6s - loss: 8.7163e-04 - val_loss: 8.4934e-04\n",
            "Epoch 94/100\n",
            " - 6s - loss: 8.5693e-04 - val_loss: 8.7069e-04\n",
            "Epoch 95/100\n",
            " - 6s - loss: 8.7000e-04 - val_loss: 8.3923e-04\n",
            "Epoch 96/100\n",
            " - 6s - loss: 8.6209e-04 - val_loss: 8.7191e-04\n",
            "Epoch 97/100\n",
            " - 6s - loss: 8.7765e-04 - val_loss: 8.6420e-04\n",
            "Epoch 98/100\n",
            " - 6s - loss: 8.6683e-04 - val_loss: 8.6089e-04\n",
            "Epoch 99/100\n",
            " - 6s - loss: 8.6576e-04 - val_loss: 8.2733e-04\n",
            "Epoch 100/100\n",
            " - 6s - loss: 8.6656e-04 - val_loss: 8.2801e-04\n",
            "Test r2 score: 0.9418584675426599\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPrEH-s9dx4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_y = model.predict(test_x)\n",
        "score = r2_score(test_y, pred_y)\n",
        "print(f'Test r2 score: {score}')\n",
        "\n",
        "#model.save('final'+model_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6BMzGxIdeuu",
        "colab_type": "text"
      },
      "source": [
        "NOW FOR PREDICTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvZk03Whn6dD",
        "colab_type": "code",
        "outputId": "bada21ad-e132-4497-97bf-78c20c9f45ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#load saved model\n",
        "from keras.models import load_model\n",
        "model_to_load = 'finalnn_model_PageRank1context.h5'\n",
        "saved_model = load_model(model_to_load)\n",
        "pred_y = saved_model.predict(test_x)\n",
        "score = r2_score(test_y, pred_y)\n",
        "print(f'Test r2 score: {score}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test r2 score: 0.9230956554054428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5SCd257CS7E",
        "colab_type": "code",
        "outputId": "198d792f-84d0-46f6-8e3e-0a0d3eed2034",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n"
      ],
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test r2 score: 0.9418584675426599\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-x1BEeFNOYj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "outputId": "cfa473d9-357f-42b3-a107-41e2f7edea92"
      },
      "source": [
        "test_mini_df = read_dataset('PageRank_test_jb_mini_test_1.csv')\n",
        "#jobs = test_mini_df['job']\n",
        "#del test_mini_df['job']\n",
        "\n",
        "#get original test_df\n",
        "test_df['flow_size'] = pred_y\n",
        "#print(test_df.tail())\n",
        "unscaled_test_df = scaler.inverse_transform(test_df)\n",
        "unscaled_test_df = pd.DataFrame(unscaled_test_df, columns = cols)\n",
        "#change_cols = {\"agg_net_out\":'int32', \"agg_net_in\":'int32', \"agg_net_out_per_machine\":'int32','agg_net_in_per_machine':'int32','machine':'int32','flow_size':'int32'}\n",
        "#unscaled_test_df = unscaled_test_df.astype(change_cols)\n",
        "unscaled_test_df.tail(20)\n"
      ],
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>flow_frequency</th>\n",
              "      <th>agg_net_out</th>\n",
              "      <th>agg_net_in</th>\n",
              "      <th>agg_net_out_per_machine</th>\n",
              "      <th>agg_net_in_per_machine</th>\n",
              "      <th>machine</th>\n",
              "      <th>flow_size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14794</th>\n",
              "      <td>102.15</td>\n",
              "      <td>7.65</td>\n",
              "      <td>190811027.0</td>\n",
              "      <td>767966981.0</td>\n",
              "      <td>1811577.0</td>\n",
              "      <td>1687049.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1002.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14795</th>\n",
              "      <td>102.25</td>\n",
              "      <td>0.10</td>\n",
              "      <td>191735863.0</td>\n",
              "      <td>767980802.0</td>\n",
              "      <td>7895583.0</td>\n",
              "      <td>178595.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1002.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14796</th>\n",
              "      <td>102.45</td>\n",
              "      <td>0.15</td>\n",
              "      <td>193350507.0</td>\n",
              "      <td>768003686.0</td>\n",
              "      <td>8375499.0</td>\n",
              "      <td>179687.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1002.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14797</th>\n",
              "      <td>102.55</td>\n",
              "      <td>0.10</td>\n",
              "      <td>194156699.0</td>\n",
              "      <td>768016991.0</td>\n",
              "      <td>8532039.0</td>\n",
              "      <td>179739.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1002.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14798</th>\n",
              "      <td>102.65</td>\n",
              "      <td>0.10</td>\n",
              "      <td>194912748.0</td>\n",
              "      <td>768027205.0</td>\n",
              "      <td>8794443.0</td>\n",
              "      <td>180467.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1002.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14799</th>\n",
              "      <td>102.85</td>\n",
              "      <td>0.20</td>\n",
              "      <td>196427263.0</td>\n",
              "      <td>768052271.0</td>\n",
              "      <td>9056847.0</td>\n",
              "      <td>181351.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1002.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14800</th>\n",
              "      <td>103.00</td>\n",
              "      <td>0.10</td>\n",
              "      <td>198154419.0</td>\n",
              "      <td>768072334.0</td>\n",
              "      <td>9844059.0</td>\n",
              "      <td>183587.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1002.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14801</th>\n",
              "      <td>103.20</td>\n",
              "      <td>1.05</td>\n",
              "      <td>200738883.0</td>\n",
              "      <td>768103241.0</td>\n",
              "      <td>1813039.0</td>\n",
              "      <td>1687273.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1002.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14802</th>\n",
              "      <td>103.40</td>\n",
              "      <td>0.10</td>\n",
              "      <td>202968838.0</td>\n",
              "      <td>768130312.0</td>\n",
              "      <td>12349199.0</td>\n",
              "      <td>192427.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1002.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14803</th>\n",
              "      <td>107.20</td>\n",
              "      <td>4.00</td>\n",
              "      <td>239656636.0</td>\n",
              "      <td>768647476.0</td>\n",
              "      <td>1814501.0</td>\n",
              "      <td>1687497.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1002.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14804</th>\n",
              "      <td>107.80</td>\n",
              "      <td>0.60</td>\n",
              "      <td>245917237.0</td>\n",
              "      <td>768735687.0</td>\n",
              "      <td>1815963.0</td>\n",
              "      <td>1687721.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1002.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14805</th>\n",
              "      <td>109.50</td>\n",
              "      <td>1.70</td>\n",
              "      <td>266029583.0</td>\n",
              "      <td>768990194.0</td>\n",
              "      <td>1820029.0</td>\n",
              "      <td>1687773.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1002.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14806</th>\n",
              "      <td>111.10</td>\n",
              "      <td>1.60</td>\n",
              "      <td>297423788.0</td>\n",
              "      <td>769268473.0</td>\n",
              "      <td>1821605.0</td>\n",
              "      <td>1687825.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1002.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14807</th>\n",
              "      <td>111.25</td>\n",
              "      <td>0.15</td>\n",
              "      <td>301173758.0</td>\n",
              "      <td>769291201.0</td>\n",
              "      <td>1828480.0</td>\n",
              "      <td>1688083.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1002.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14808</th>\n",
              "      <td>112.65</td>\n",
              "      <td>1.40</td>\n",
              "      <td>301187008.0</td>\n",
              "      <td>769292873.0</td>\n",
              "      <td>1839536.0</td>\n",
              "      <td>1688635.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1002.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14809</th>\n",
              "      <td>113.15</td>\n",
              "      <td>0.50</td>\n",
              "      <td>301215840.0</td>\n",
              "      <td>769359493.0</td>\n",
              "      <td>1868368.0</td>\n",
              "      <td>1755255.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1002.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14810</th>\n",
              "      <td>113.30</td>\n",
              "      <td>0.15</td>\n",
              "      <td>301235805.0</td>\n",
              "      <td>769383028.0</td>\n",
              "      <td>1888333.0</td>\n",
              "      <td>1778790.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1002.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14811</th>\n",
              "      <td>113.60</td>\n",
              "      <td>0.25</td>\n",
              "      <td>301255874.0</td>\n",
              "      <td>769383288.0</td>\n",
              "      <td>1908402.0</td>\n",
              "      <td>1779050.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1002.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14812</th>\n",
              "      <td>113.75</td>\n",
              "      <td>38.40</td>\n",
              "      <td>301267361.0</td>\n",
              "      <td>769383444.0</td>\n",
              "      <td>81907.0</td>\n",
              "      <td>155879.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1002.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14813</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1002.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         time  flow_frequency  ...  machine  flow_size\n",
              "14794  102.15            7.65  ...      4.0     1002.0\n",
              "14795  102.25            0.10  ...      5.0     1002.0\n",
              "14796  102.45            0.15  ...      5.0     1002.0\n",
              "14797  102.55            0.10  ...      5.0     1002.0\n",
              "14798  102.65            0.10  ...      5.0     1002.0\n",
              "14799  102.85            0.20  ...      5.0     1002.0\n",
              "14800  103.00            0.10  ...      5.0     1002.0\n",
              "14801  103.20            1.05  ...      4.0     1002.0\n",
              "14802  103.40            0.10  ...      5.0     1002.0\n",
              "14803  107.20            4.00  ...      4.0     1002.0\n",
              "14804  107.80            0.60  ...      4.0     1002.0\n",
              "14805  109.50            1.70  ...      4.0     1002.0\n",
              "14806  111.10            1.60  ...      4.0     1002.0\n",
              "14807  111.25            0.15  ...      4.0     1002.0\n",
              "14808  112.65            1.40  ...      4.0     1002.0\n",
              "14809  113.15            0.50  ...      4.0     1002.0\n",
              "14810  113.30            0.15  ...      4.0     1002.0\n",
              "14811  113.60            0.25  ...      4.0     1002.0\n",
              "14812  113.75           38.40  ...      0.0     1002.0\n",
              "14813    0.00            0.00  ...      0.0     1002.0\n",
              "\n",
              "[20 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_N3E_AvPKag",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "418aa018-fa4f-4b27-94ac-74592cde4dee"
      },
      "source": [
        "#just to see max and min flow sizes\n",
        "#unscaled_test_df.loc[unscaled_test_df['flow_size'].idxmax()]\n",
        "#unscaled_test_df.loc[1942]['flow_size']"
      ],
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "time                       1.385500e+02\n",
              "flow_frequency             1.320000e+01\n",
              "agg_net_out                1.027385e+08\n",
              "agg_net_in                 1.369980e+09\n",
              "agg_net_out_per_machine    3.154502e+06\n",
              "agg_net_in_per_machine     3.080607e+06\n",
              "machine                    4.000000e+00\n",
              "flow_size                  4.114875e+08\n",
              "Name: 1942, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 267
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv75zRlROXrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_df = pd.DataFrame([])\n",
        "new_df['flow_size'] = test_mini_df['flow_size']\n",
        "new_df['job'] = test_mini_df['job']\n",
        "new_df['predicted_flow_size'] = test_mini_df['flow_size']\n",
        "\n",
        "#now copy predicted\n",
        "for index, row in test_mini_df.iterrows():\n",
        "  i = int(row['pseudo_index'])\n",
        "  new_df.iloc[index,2] = unscaled_test_df.iloc[i]['flow_size']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wq91FTgd75e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save\n",
        "new_df.to_csv('PageRank_test_jb_mini_predicted_no_outliers.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wbQ_abZCvNN",
        "colab_type": "code",
        "outputId": "d524645f-6c85-4d8f-e207-f4a6e5a6f80a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "new_df.head(10)"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>flow_size</th>\n",
              "      <th>job</th>\n",
              "      <th>predicted_flow_size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1576.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1002.000001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2777.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1002.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4763.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1002.000215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1494.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1002.001862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>30717.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1002.001983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3560.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1002.002916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>15372.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1002.001450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>22817.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1002.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2061.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1002.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1576.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1002.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   flow_size  job  predicted_flow_size\n",
              "0     1576.0    1          1002.000001\n",
              "1     2777.0    1          1002.000002\n",
              "2     4763.0    1          1002.000215\n",
              "3     1494.0    1          1002.001862\n",
              "4    30717.0    1          1002.001983\n",
              "5     3560.0    1          1002.002916\n",
              "6    15372.0    1          1002.001450\n",
              "7    22817.0    1          1002.000000\n",
              "8     2061.0    1          1002.000000\n",
              "9     1576.0    1          1002.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AktG5KnD9wXq",
        "colab_type": "code",
        "outputId": "8358434a-82b5-443e-9c06-b946fa4fbee0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "test_mini_df.head()"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pseudo_index</th>\n",
              "      <th>time</th>\n",
              "      <th>flow_frequency</th>\n",
              "      <th>agg_net_out</th>\n",
              "      <th>agg_net_in</th>\n",
              "      <th>agg_net_out_per_machine</th>\n",
              "      <th>agg_net_in_per_machine</th>\n",
              "      <th>machine</th>\n",
              "      <th>flow_size</th>\n",
              "      <th>job</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>14.50</td>\n",
              "      <td>4.80</td>\n",
              "      <td>1576.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1576.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>4</td>\n",
              "      <td>1576.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>16.25</td>\n",
              "      <td>1.75</td>\n",
              "      <td>4353.0</td>\n",
              "      <td>3078.0</td>\n",
              "      <td>4353.0</td>\n",
              "      <td>3078.0</td>\n",
              "      <td>4</td>\n",
              "      <td>2777.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>16.55</td>\n",
              "      <td>0.15</td>\n",
              "      <td>9116.0</td>\n",
              "      <td>5207.0</td>\n",
              "      <td>9116.0</td>\n",
              "      <td>5207.0</td>\n",
              "      <td>4</td>\n",
              "      <td>4763.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>16.80</td>\n",
              "      <td>0.25</td>\n",
              "      <td>10610.0</td>\n",
              "      <td>5431.0</td>\n",
              "      <td>10610.0</td>\n",
              "      <td>5431.0</td>\n",
              "      <td>4</td>\n",
              "      <td>1494.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>17.10</td>\n",
              "      <td>0.10</td>\n",
              "      <td>41327.0</td>\n",
              "      <td>72699.0</td>\n",
              "      <td>41327.0</td>\n",
              "      <td>72699.0</td>\n",
              "      <td>4</td>\n",
              "      <td>30717.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pseudo_index   time  flow_frequency  ...  machine  flow_size  job\n",
              "0             0  14.50            4.80  ...        4     1576.0    1\n",
              "1             1  16.25            1.75  ...        4     2777.0    1\n",
              "2             2  16.55            0.15  ...        4     4763.0    1\n",
              "3             3  16.80            0.25  ...        4     1494.0    1\n",
              "4             4  17.10            0.10  ...        4    30717.0    1\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD2nEl4o-QvK",
        "colab_type": "code",
        "outputId": "1b561aec-c66b-4d40-a3a5-57b77649b715",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "unscaled_test_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>flow_frequency</th>\n",
              "      <th>agg_net_out</th>\n",
              "      <th>agg_net_in</th>\n",
              "      <th>agg_net_out_per_machine</th>\n",
              "      <th>agg_net_in_per_machine</th>\n",
              "      <th>machine</th>\n",
              "      <th>flow_size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14.50</td>\n",
              "      <td>4.80</td>\n",
              "      <td>1576</td>\n",
              "      <td>52</td>\n",
              "      <td>1576</td>\n",
              "      <td>52</td>\n",
              "      <td>4</td>\n",
              "      <td>1576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16.25</td>\n",
              "      <td>1.75</td>\n",
              "      <td>4353</td>\n",
              "      <td>3078</td>\n",
              "      <td>4353</td>\n",
              "      <td>3078</td>\n",
              "      <td>4</td>\n",
              "      <td>2777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16.55</td>\n",
              "      <td>0.15</td>\n",
              "      <td>9116</td>\n",
              "      <td>5207</td>\n",
              "      <td>9116</td>\n",
              "      <td>5207</td>\n",
              "      <td>4</td>\n",
              "      <td>4762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16.80</td>\n",
              "      <td>0.25</td>\n",
              "      <td>10610</td>\n",
              "      <td>5431</td>\n",
              "      <td>10610</td>\n",
              "      <td>5431</td>\n",
              "      <td>4</td>\n",
              "      <td>1494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.10</td>\n",
              "      <td>0.10</td>\n",
              "      <td>41327</td>\n",
              "      <td>72699</td>\n",
              "      <td>41327</td>\n",
              "      <td>72699</td>\n",
              "      <td>4</td>\n",
              "      <td>30717</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    time  flow_frequency  ...  machine  flow_size\n",
              "0  14.50            4.80  ...        4       1576\n",
              "1  16.25            1.75  ...        4       2777\n",
              "2  16.55            0.15  ...        4       4762\n",
              "3  16.80            0.25  ...        4       1494\n",
              "4  17.10            0.10  ...        4      30717\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    }
  ]
}